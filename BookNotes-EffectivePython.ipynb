{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Item 1: Know Which Version of Python You’re Using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=5, micro=1, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)\n",
    "\n",
    "# python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are two major versions of Python still in active use: Python 2 and Python 3.\n",
    "2. There are multiple popular runtimes for Python: CPython, Jython, IronPython, PyPy, etc.\n",
    "3. Be sure that the command-line for running Python on your system is the version you expect it to be.\n",
    "4. Prefer Python 3 for your next project because that is the primary focus of the Python community."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 2: Follow the PEP 8 Style Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Enhancement Proposal #8, otherwise known as PEP 8, is the style guide for how to format Python code. You are welcome to write Python code however you want, as long as it has valid syntax. However, using a consistent style makes your code more approachable and easier to read. Sharing a common style with other Python programmers in the larger community facilitates collaboration on projects. But even if you are the only one who will ever read your code, following the style guide will make it easier to change things later.\n",
    "\n",
    "PEP 8 has a wealth of details about how to write clear Python code. It continues to be updated as the Python language evolves. It’s worth reading the whole guide online (http://www.python.org/dev/peps/pep-0008/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pylint tool (http://www.pylint.org/) is a popular static analyzer for Python source code. Pylint provides automated enforcement of the PEP 8 style guide and detects many other types of common errors in Python programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 3: Know the Differences Between bytes, str, and unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python 3, there are two types that represent sequences of characters: bytes and str. Instances of bytes contain raw 8-bit values. Instances of str contain Unicode characters.\n",
    "\n",
    "In Python 2, there are two types that represent sequences of characters: str and unicode. In contrast to Python 3, instances of str contain raw 8-bit values. Instances of unicode contain Unicode characters.\n",
    "\n",
    "There are many ways to represent Unicode characters as binary data (raw 8-bit values). The most common encoding is UTF-8. Importantly, str instances in Python 3 and unicode instances in Python 2 do not have an associated binary encoding. \n",
    "\n",
    "** To convert Unicode characters to binary data, you must use the encode method.**\n",
    "\n",
    "** To convert binary data to Unicode characters, you must use the decode method.**\n",
    "\n",
    "When you’re writing Python programs, it’s important to do encoding and decoding of Unicode at the furthest boundary of your interfaces. The core of your program should use Unicode character types (str in Python 3, unicode in Python 2) and should not assume anything about character encodings. This approach allows you to be very accepting of alternative text encodings (such as Latin-1, Shift JIS, and Big5) while being strict about your output text encoding (ideally, UTF-8).\n",
    "\n",
    "The split between character types leads to two common situations in Python code:\n",
    "You want to operate on raw 8-bit values that are UTF-8-encoded characters (or some other encoding).\n",
    "You want to operate on Unicode characters that have no specific encoding.\n",
    "You’ll often need two helper functions to convert between these two cases and to ensure that the type of input values matches your code’s expectations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_str(bytes_or_str):\n",
    "       if isinstance(bytes_or_str, bytes):\n",
    "           value = bytes_or_str.decode('utf-8')\n",
    "       else:\n",
    "           value = bytes_or_str\n",
    "       return value  # Instance of str\n",
    "\n",
    "def to_bytes(bytes_or_str):\n",
    "       if isinstance(bytes_or_str, str):\n",
    "           value = bytes_or_str.encode('utf-8')\n",
    "       else:\n",
    "           value = bytes_or_str\n",
    "       return value  # Instance of bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 4: Write Helper Functions Instead of Complex Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As soon as your expressions get complicated, it’s time to consider splitting them into smaller pieces and moving logic into helper functions. What you gain in readability always outweighs what brevity may have afforded you. Don’t let Python’s pithy syntax for complex expressions get you into a mess like this.\n",
    "\n",
    "```\n",
    "\n",
    "from urllib.parse import parse_qs\n",
    "my_values = parse_qs(‘red=5&blue=0&green=’,\n",
    "                        keep_blank_values=True)\n",
    "print(repr(my_values))\n",
    ">>>{‘red’: [‘5’], ‘green’: [”], ‘blue’: [‘0’]}\n",
    "\n",
    "\n",
    "red = int(my_values.get(‘red’, [”])[0] or 0)\n",
    "\n",
    "red = int(red[0]) if red[0] else 0\n",
    "\n",
    "def get_first_int(values, key, default=0):\n",
    "       found = values.get(key, [”])\n",
    "       if found[0]:\n",
    "           found = int(found[0])\n",
    "       else:\n",
    "           found = default\n",
    "       return found\n",
    "\n",
    "red = get_first_int(my_values, ‘green’)\n",
    "\n",
    "```\n",
    "\n",
    "1. Python’s syntax makes it all too easy to write single-line expressions that are overly complicated and difficult to read.\n",
    "2. Move complex expressions into helper functions, especially if you need to use the same logic repeatedly.\n",
    "\n",
    "3. The if/else expression provides a more readable alternative to using Boolean operators like or and and in expressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 5: Know How to Slice Sequences\n",
    "\n",
    "Python includes syntax for slicing sequences into pieces. Slicing lets you access a subset of a sequence’s items with minimal effort. The simplest uses for slicing are the built-in types list, str, and bytes. Slicing can be extended to any Python class that implements the __getitem__ and __setitem__ special methods\n",
    "\n",
    "** The basic form of the slicing syntax is somelist[start:end], where start is inclusive and end is exclusive.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First four: ['a', 'b', 'c', 'd']\n",
      "Last four:  ['e', 'f', 'g', 'h']\n",
      "Middle two: ['d', 'e']\n"
     ]
    }
   ],
   "source": [
    "a = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "print('First four:', a[:4])\n",
    "print('Last four: ', a[-4:])\n",
    "print('Middle two:', a[3:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When slicing from the start of a list, you should leave out the zero index to reduce visual noise.\n",
    "   assert a[:5] == a[0:5]\n",
    "\n",
    "* When slicing to the end of a list, you should leave out the final index because it’s redundant.\n",
    "    assert a[5:] == a[5:len(a)]\n",
    "\n",
    "* Slicing deals properly with start and end indexes that are beyond the boundaries of the list. That makes it easy for your code to establish a maximum length to consider for an input sequence.\n",
    "   first_twenty_items = a[:20]\n",
    "   last_twenty_items = a[-20:]\n",
    "\n",
    "* In contrast, accessing the same index directly causes an exception.\n",
    "```\n",
    " >>> a[20] \n",
    "   IndexError: list index out of range\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of slicing a list is a whole new list. References to the objects from the original list are maintained. Modifying the result of slicing won’t affect the original list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:   ['e', 'f', 'g', 'h']\n",
      "After:     ['e', 99, 'g', 'h']\n",
      "No change: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n"
     ]
    }
   ],
   "source": [
    "b = a[4:]\n",
    "print('Before:  ', b)\n",
    "b[1] = 99\n",
    "print('After:    ', b)\n",
    "print('No change:', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avoid being verbose: Don’t supply 0 for the start index or the length of the sequence for the end index.\n",
    "\n",
    "* Slicing is forgiving of start or end indexes that are out of bounds, making it easy to express slices on the front or back boundaries of a sequence (like a[:20] or a[-20:]).\n",
    "\n",
    "* Assigning to a list slice will replace that range in the original sequence with what’s referenced even if their lengths are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 6: Avoid Using start, end, and stride in a Single Slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to basic slicing (see Item 5: “Know How to Slice Sequences”), Python has special syntax for the stride of a slice in the form somelist[start:end:stride]. This lets you take every nth item when slicing a sequence. For example, the stride makes it easy to group by even and odd indexes in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'yellow', 'blue']\n",
      "['orange', 'green', 'purple']\n"
     ]
    }
   ],
   "source": [
    "   a = ['red', 'orange', 'yellow', 'green', 'blue', 'purple']\n",
    "   odds = a[::2]\n",
    "   evens = a[1::2]\n",
    "   print(odds)\n",
    "   print(evens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is that the stride part of the slicing syntax can be extremely confusing. Having three numbers within the brackets is hard enough to read because of its density. Then it’s not obvious when the start and end indexes come into effect relative to the stride value, especially when stride is negative.\n",
    "To prevent problems, avoid using stride along with start and end indexes. If you must use a stride, prefer making it a positive value and omit start and end indexes. If you must use stride with start or end indexes, consider using one assignment to stride and another to slice.\n",
    "```\n",
    "    ￼b = a[::2]   # [‘a’, ‘c’, ‘e’, ‘g’]\n",
    "    c = b[1:-1]  # [‘c’, ‘e’]\n",
    "```\n",
    "Slicing and then striding will create an extra shallow copy of the data. The first operation should try to reduce the size of the resulting slice by as much as possible. If your program can’t afford the time or memory required for two steps, consider using the itertools built-in module’s islice method (see Item 46: “Use Built-in Algorithms and Data Structures”), which doesn’t permit negative values for start, end, or stride."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Specifying start, end, and stride in a slice can be extremely confusing.\n",
    "* Prefer using positive stride values in slices without start or end indexes. Avoid negative stride values if possible.\n",
    "* Avoid using start, end, and stride together in a single slice. If you need all three parameters, consider doing two assignments (one to slice, another to stride) or using islice from the itertools built-in module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 7: Use List Comprehensions Instead of map and filter\n",
    "\n",
    "Python provides compact syntax for deriving one list from another. These expressions are called list comprehensions. For example, say you want to compute the square of each number in a list. You can do this by providing the expression for your computation and the input sequence to loop over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "squares = [x**2 for x in a]\n",
    "print(squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless you’re applying a single-argument function, list comprehensions are clearer than the map built-in function for simple cases. map requires creating a lambda function for the computation, which is visually noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squares = map(lambda x: x ** 2, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike map, list comprehensions let you easily filter items from the input list, removing corresponding outputs from the result. For example, say you only want to compute the squares of the numbers that are divisible by 2. Here, I do this by adding a conditional expression to the list comprehension after the loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 16, 36, 64, 100]\n"
     ]
    }
   ],
   "source": [
    "even_squares = [x**2 for x in a if x % 2 == 0]\n",
    "print(even_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter built-in function can be used along with map to achieve the same outcome, but it is much harder to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alt = map(lambda x: x**2, filter(lambda x: x % 2 == 0, a))\n",
    "assert even_squares == list(alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries and sets have their own equivalents of list comprehensions. These make it easy to create derivative data structures when writing algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'ghost', 2: 'habanero', 3: 'cayenne'}\n",
      "{8, 5, 7}\n"
     ]
    }
   ],
   "source": [
    "chile_ranks = {'ghost': 1, 'habanero': 2, 'cayenne': 3}\n",
    "rank_dict = {rank: name for name, rank in chile_ranks.items()}\n",
    "chile_len_set = {len(name) for name in rank_dict.values()}\n",
    "print(rank_dict)\n",
    "print(chile_len_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* List comprehensions are clearer than the map and filter built-in functions because they don’t require extra lambda expressions.\n",
    "* List comprehensions allow you to easily skip items from the input list, a behavior map doesn’t support without help from filter.\n",
    "* Dictionaries and sets also support comprehension expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 8: Avoid More Than Two Expressions in List Comprehensions\n",
    "\n",
    "Beyond basic usage (see Item 7: “Use List Comprehensions Instead of map and filter”), list comprehensions also support multiple levels of looping. For example, say you want to simplify a matrix (a list containing other lists) into one flat list of all cells. Here, I do this with a list comprehension by including two for expressions. These expressions run in the order provided from left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "flat = [x for row in matrix for x in row]\n",
    "print(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is simple, readable, and a reasonable usage of multiple loops. Another reasonable usage of multiple loops is replicating the two-level deep layout of the input list.\n",
    "\n",
    "For example, say you want to square the value in each cell of a two-dimensional matrix. This expression is noisier because of the extra [] characters, but it’s still easy to read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 9], [16, 25, 36], [49, 64, 81]]\n"
     ]
    }
   ],
   "source": [
    "squared = [[x**2 for x in row] for row in matrix]\n",
    "print(squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this expression included another loop, the list comprehension would get so long that you’d have to split it over multiple lines.\n",
    "```\n",
    "my_lists = [\n",
    "[[1, 2, 3], [4, 5, 6]], #...\n",
    "   ]\n",
    "   flat = [x for sublist1 in my_lists\n",
    "           for sublist2 in sublist1\n",
    "           for x in sublist2]\n",
    "           ```\n",
    "At this point, the multiline comprehension isn’t much shorter than the alternative. Here, I produce the same result using normal loop statements. The indentation of this version makes the looping clearer than the list comprehension.\n",
    "  ```\n",
    "  flat = []\n",
    "   for sublist1 in my_lists:\n",
    "       for sublist2 in sublist1:\n",
    "           flat.extend(sublist2)\n",
    "           ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions also support multiple if conditions. Multiple conditions at the same loop level are an implicit and expression. For example, say you want to filter a list of numbers to only even values greater than four. These two list comprehensions are equivalent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 8, 10]\n",
      "[6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "b = [x for x in a if x > 4 if x % 2 == 0]\n",
    "c = [x for x in a if x > 4 and x % 2 == 0]\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditions can be specified at each level of looping after the for expression. For example, say you want to filter a matrix so the only cells remaining are those divisible by 3 in rows that sum to 10 or higher. Expressing this with list comprehensions is short, but extremely difficult to read.\n",
    "```\n",
    "   matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "   filtered = [[x for x in row if x % 3 == 0]\n",
    "               for row in matrix if sum(row) >= 10]\n",
    "   print(filtered)\n",
    "   >>>\n",
    "   [[6], [9]]\n",
    "   ```\n",
    "Though this example is a bit convoluted, in practice you’ll see situations arise where such expressions seem like a good fit. I strongly encourage you to avoid using list\n",
    "￼￼￼￼\n",
    "comprehensions that look like this. The resulting code is very difficult for others to comprehend. What you save in the number of lines doesn’t outweigh the difficulties it could cause later.\n",
    "The rule of thumb is to avoid using more than two expressions in a list comprehension. This could be two conditions, two loops, or one condition and one loop. As soon as it gets more complicated than that, you should use normal if and for statements and write a helper function (see Item 16: “Consider Generators Instead of Returning Lists”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List comprehensions support multiple levels of loops and multiple conditions per loop level.\n",
    "* List comprehensions with more than two expressions are very difficult to read and should be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 9: Consider Generator Expressions for Large Comprehensions\n",
    "\n",
    "The problem with list comprehensions (see Item 7: “Use List Comprehensions Instead of map and filter”) is that they may create a whole new list containing one item for each value in the input sequence. This is fine for small inputs, but for large inputs this could consume significant amounts of memory and cause your program to crash.\n",
    "\n",
    "For example, say you want to read a file and return the number of characters on each line. Doing this with a list comprehension would require holding the length of every line of the file in memory. If the file is absolutely enormous or perhaps a never-ending network socket, list comprehensions are problematic. Here, I use a list comprehension in a way that can only handle small input values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "value = [len(x) for x in open(‘/tmp/my_file.txt’)]\n",
    "   print(value)\n",
    "   >>>\n",
    "   [100, 57, 15, 1, 12, 75, 5, 86, 89, 11]\n",
    "```\n",
    "To solve this, Python provides generator expressions, a generalization of list comprehensions and generators. Generator expressions don’t materialize the whole output sequence when they’re run. Instead, generator expressions evaluate to an iterator that yields one item at a time from the expression.\n",
    "A generator expression is created by putting list-comprehension-like syntax between () characters. Here, I use a generator expression that is equivalent to the code above. However, the generator expression immediately evaluates to an iterator and doesn’t make any forward progress.\n",
    "\n",
    "```\n",
    "   it = (len(x) for x in open(‘/tmp/my_file.txt’))\n",
    "   print(it)\n",
    "￼￼￼￼￼￼￼￼￼￼￼\n",
    ">>>\n",
    "   <generator object <genexpr> at 0x101b81480>\n",
    "```\n",
    "\n",
    "The returned iterator can be advanced one step at a time to produce the next output from the generator expression as needed (using the next built-in function). Your code can consume as much of the generator expression as you want without risking a blowup in memory usage.\n",
    "```\n",
    "   print(next(it))\n",
    "   print(next(it))\n",
    ">>> 100 57\n",
    "```\n",
    "Another powerful outcome of generator expressions is that they can be composed together. Here, I take the iterator returned by the generator expression above and use it as the input for another generator expression.\n",
    "```\n",
    "roots = ((x, x**0.5) for x in it)\n",
    "```\n",
    "Each time I advance this iterator, it will also advance the interior iterator, creating a domino effect of looping, evaluating conditional expressions, and passing around inputs and outputs.\n",
    "```\n",
    "   print(next(roots))\n",
    "   >>>\n",
    "   (15, 3.872983346207417)\n",
    "```\n",
    "Chaining generators like this executes very quickly in Python. When you’re looking for a way to compose functionality that’s operating on a large stream of input, generator expressions are the best tool for the job. The only gotcha is that the iterators returned by generator expressions are stateful, so you must be careful not to use them more than once (see Item 17: “Be Defensive When Iterating Over Arguments”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List comprehensions can cause problems for large inputs by using too much memory.\n",
    "* Generator expressions avoid memory issues by producing outputs one at a time as an iterator.\n",
    "* Generator expressions can be composed by passing the iterator from one generator expression into the for subexpression of another.\n",
    "* Generator expressions execute very quickly when chained together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 10: Prefer enumerate Over range\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range built-in function is useful for loops that iterate over a set of integers.\n",
    "```\n",
    "random_bits = 0\n",
    "   for i in range(64):\n",
    "￼￼￼￼￼￼￼\n",
    "if randint(0, 1):\n",
    "           random_bits |= 1 << i\n",
    "```\n",
    "When you have a data structure to iterate over, like a list of strings, you can loop directly over the sequence.\n",
    "\n",
    "```\n",
    "flavor_list = [‘vanilla’, ‘chocolate’, ‘pecan’, ‘strawberry’]\n",
    "   for flavor in flavor_list:\n",
    "       print(‘%s is delicious’ % flavor)\n",
    "```\n",
    "Often, you’ll want to iterate over a list and also know the index of the current item in the list. For example, say you want to print the ranking of your favorite ice cream flavors. One way to do it is using range.\n",
    "\n",
    "```\n",
    "   for i in range(len(flavor_list)):\n",
    "       flavor = flavor_list[i]\n",
    "       print(‘%d: %s’ % (i + 1, flavor))\n",
    "```\n",
    "This looks clumsy compared with the other examples of iterating over flavor_list or range. You have to get the length of the list. You have to index into the array. It’s harder to read.\n",
    "Python provides the enumerate built-in function for addressing this situation. enumerate wraps any iterator with a lazy generator. This generator yields pairs of the loop index and the next value from the iterator. The resulting code is much clearer.\n",
    "\n",
    "```\n",
    "   for i, flavor in enumerate(flavor_list):\n",
    "       print(‘%d: %s’ % (i + 1, flavor))\n",
    "   >>>\n",
    "   1: vanilla\n",
    "   2: chocolate\n",
    "   3: pecan\n",
    "   4: strawberry\n",
    "```\n",
    "You can make this even shorter by specifying the number from which enumerate should begin counting (1 in this case).\n",
    "```\n",
    "   for i, flavor in enumerate(flavor_list, 1):\n",
    "       print(‘%d: %s’ % (i, flavor))\n",
    "```\n",
    "\n",
    "* enumerate provides concise syntax for looping over an iterator and getting the index of each item from the iterator as you go.\n",
    "* Prefer enumerate instead of looping over a range and indexing into a sequence.\n",
    "* You can supply a second parameter to enumerate to specify the number from which to begin counting (zero is the default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 11: Use zip to Process Iterators in Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often in Python you find yourself with many lists of related objects. List comprehensions make it easy to take a source list and get a derived list by applying an expression (see Item 7: “Use List Comprehensions Instead of map and filter”).\n",
    "```Click here to view code image\n",
    "   names = [‘Cecilia’, ‘Lise’, ‘Marie’]\n",
    "   letters = [len(n) for n in names]\n",
    "```\n",
    "The items in the derived list are related to the items in the source list by their indexes. To iterate over both lists in parallel, you can iterate over the length of the names source list.\n",
    "```Click here to view code image\n",
    "   longest_name = None\n",
    "   max_letters = 0\n",
    "   for i in range(len(names)):\n",
    "       count = letters[i]\n",
    "       if count > max_letters:\n",
    "           longest_name = names[i]\n",
    "           max_letters = count\n",
    "   print(longest_name)\n",
    "   >>>\n",
    "Cecilia\n",
    "```\n",
    "The problem is that this whole loop statement is visually noisy. The indexes into names and letters make the code hard to read. Indexing into the arrays by the loop index i happens twice. Using enumerate (see Item 10: “Prefer enumerate Over range”) improves this slightly, but it’s still not ideal.\n",
    "```Click here to view code image\n",
    "   for i, name in enumerate(names):\n",
    "       count = letters[i]\n",
    "       if count > max_letters:\n",
    "           longest_name = name\n",
    "           max_letters = count\n",
    "           \n",
    "```\n",
    "To make this code clearer, Python provides the zip built-in function. In Python 3, zip wraps two or more iterators with a lazy generator. The zip generator yields tuples containing the next value from each iterator. The resulting code is much cleaner than indexing into multiple lists.\n",
    "\n",
    "```Click here to view code image\n",
    "   for name, count in zip(names, letters):\n",
    "       if count > max_letters:\n",
    "           longest_name = name\n",
    "           max_letters = count\n",
    "```\n",
    "There are two problems with the zip built-in.\n",
    "The first issue is that in Python 2 zip is not a generator; it will fully exhaust the supplied iterators and return a list of all the tuples it creates. This could potentially use a lot of memory and cause your program to crash. If you want to zip very large iterators in\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼\n",
    "Python 2, you should use izip from the itertools built-in module (see Item 46: “Use Built-in Algorithms and Data Structures”).\n",
    "The second issue is that zip behaves strangely if the input iterators are of different lengths. For example, say you add another name to the list above but forget to update the letter counts. Running zip on the two input lists will have an unexpected result.\n",
    "\n",
    "``` \n",
    "Click here to view code image\n",
    "   names.append(‘Rosalind’)\n",
    "   for name, count in zip(names, letters):\n",
    "   print(name)\n",
    "   >>>\n",
    "   Cecilia\n",
    "   Lise\n",
    "   Marie\n",
    "   \n",
    "```\n",
    "The new item for 'Rosalind' isn’t there. This is just how zip works. It keeps yielding tuples until a wrapped iterator is exhausted. This approach works fine when you know that the iterators are of the same length, which is often the case for derived lists created by list comprehensions. In many other cases, the truncating behavior of zip is surprising and bad. If you aren’t confident that the lengths of the lists you want to zip are equal, consider using the zip_longest function from the itertools built-in module instead (also called izip_longest in Python 2).\n",
    "\n",
    "* The zip built-in function can be used to iterate over multiple iterators in parallel.\n",
    "* In Python 3, zip is a lazy generator that produces tuples. In Python 2, zip returns the full result as a list of tuples.\n",
    "* zip truncates its output silently if you supply it with iterators of different lengths.\n",
    "* The zip_longest function from the itertools built-in module lets you iterate over multiple iterators in parallel regardless of their lengths (see Item 46: “Use Built-in Algorithms and Data Structures”).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 12: Avoid else Blocks After for and while Loops\n",
    "\n",
    "Python loops have an extra feature that is not available in most other programming languages: you can put an else block immediately after a loop’s repeated interior block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0\n",
      "Loop 1\n",
      "Loop 2\n",
      "Else block!\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('Loop %d' % i)\n",
    "else:\n",
    "    print('Else block!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the else block runs immediately after the loop finishes. Why is the clause called “else”? Why not “and”? In an if/else statement, else means, “Do this if the block before this doesn’t happen.” In a try/except statement, except has the same definition: “Do this if trying the block before this failed.”\n",
    "\n",
    "Similarly, else from try/except/else follows this pattern (see Item 13: “Take Advantage of Each Block in try/except/else/finally”) because it means, “Do this if the block before did not fail.” try/finally is also intuitive because it means, “Always do what is final after trying the block before.”\n",
    "\n",
    "Given all of the uses of else, except, and finally in Python, a new programmer might assume that the else part of for/else means, “Do this if the loop wasn’t completed.” In reality, it does exactly the opposite. Using a break statement in a loop will actually skip the else block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop 0\n",
      "Loop 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "       print('Loop %d' % i)\n",
    "       if i == 1:\n",
    "        break \n",
    "else:\n",
    "       print('Else block!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 13: Take Advantage of Each Block in try/except/else/finally\n",
    "\n",
    "#### Finally Blocks\n",
    "Use try/finally when you want exceptions to propagate up, but you also want to run cleanup code even when exceptions occur. One common usage of try/finally is for reliably closing file handles (see Item 43: “Consider contextlib and with Statements for Reusable try/finally Behavior” for another approach).\n",
    "```\n",
    "Click here to view code image\n",
    "   handle = open(‘/tmp/random_data.txt’)  # May raise IOError\n",
    "   try:\n",
    "       data = handle.read()  # May raise UnicodeDecodeError\n",
    "   finally:\n",
    "       handle.close()        # Always runs after try:\n",
    "       \n",
    "```\n",
    "Any exception raised by the read method will always propagate up to the calling code, yet the close method of handle is also guaranteed to run in the finally block. You must call open before the try block because exceptions that occur when opening the file (like IOError if the file does not exist) should skip the finally block.\n",
    "\n",
    "#### Else Blocks\n",
    "\n",
    "Use try/except/else to make it clear which exceptions will be handled by your code and which exceptions will propagate up. When the try block doesn’t raise an exception, the else block will run. The else block helps you minimize the amount of code in the try block and improves readability. For example, say you want to load JSON dictionary data from a string and return the value of a key it contains.\n",
    "```\n",
    "Click here to view code image\n",
    "   def load_json_key(data, key):\n",
    "       try:\n",
    "           result_dict = json.loads(data)  # May raise ValueError\n",
    "       except ValueError as e:\n",
    "           raise KeyError from e\n",
    "       else:\n",
    "           return result_dict[key]         # May raise KeyError\n",
    "```\n",
    "If the data isn’t valid JSON, then decoding with json.loads will raise a ValueError. The exception is caught by the except block and handled. If decoding is\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼\n",
    "successful, then the key lookup will occur in the else block. If the key lookup raises any exceptions, they will propagate up to the caller because they are outside the try block. The else clause ensures that what follows the try/except is visually distinguished from the except block. This makes the exception propagation behavior clear.\n",
    "\n",
    "#### Everything Together\n",
    "\n",
    "Use try/except/else/finally when you want to do it all in one compound statement. For example, say you want to read a description of work to do from a file, process it, and then update the file in place. Here, the try block is used to read the file and process it. The except block is used to handle exceptions from the try block that are expected. The else block is used to update the file in place and to allow related exceptions to propagate up. The finally block cleans up the file handle.\n",
    "```\n",
    "Click here to view code image\n",
    "   UNDEFINED = object()\n",
    "   def divide_json(path):\n",
    "       handle = open(path, ‘r+’)\n",
    "       try:\n",
    "           data = handle.read()\n",
    "           op = json.loads(data)\n",
    "           value = (\n",
    "               op[‘numerator’] /\n",
    "# May raise IOError\n",
    "# May raise UnicodeDecodeError\n",
    "# May raise ValueError\n",
    "￼        op[‘denominator’])  # May raise ZeroDivisionError\n",
    "except ZeroDivisionError as e:\n",
    "    return UNDEFINED\n",
    "else:\n",
    "    op[‘result’] = value\n",
    "    result = json.dumps(op)\n",
    "    handle.seek(0)\n",
    "    handle.write(result)\n",
    "    return value\n",
    "finally:\n",
    "    handle.close()\n",
    "# May raise IOError\n",
    "# Always runs\n",
    "\n",
    "```\n",
    "This layout is especially useful because all of the blocks work together in intuitive ways. For example, if an exception gets raised in the else block while rewriting the result data, the finally block will still run and close the file handle.\n",
    "\n",
    "\n",
    "* The try/finally compound statement lets you run cleanup code regardless of whether exceptions were raised in the try block.\n",
    "* The else block helps you minimize the amount of code in try blocks and visually distinguish the success case from the try/except blocks.\n",
    "* An else block can be used to perform additional actions after a successful try block but before common cleanup in a finally block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "### Item 14: Prefer Exceptions to Returning None\n",
    "When writing utility functions, there’s a draw for Python programmers to give special meaning to the return value of None. It seems to makes sense in some cases. For example, say you want a helper function that divides one number by another. In the case of dividing by zero, returning None seems natural because the result is undefined.\n",
    "```\n",
    "def divide(a, b):\n",
    "       try:\n",
    "           return a / b\n",
    "       except ZeroDivisionError:\n",
    "return None\n",
    "```\n",
    "Code using this function can interpret the return value accordingly.\n",
    "   ```\n",
    "   result = divide(x, y)\n",
    "   if result is None:\n",
    "       print(‘Invalid inputs’)\n",
    "       \n",
    "```\n",
    "What happens when the numerator is zero? That will cause the return value to also be zero (if the denominator is non-zero). This can cause problems when you evaluate the result in a condition like an if statement. You may accidentally look for any False equivalent value to indicate errors instead of only looking for None (see Item 4: “Write Helper Functions Instead of Complex Expressions” for a similar situation).\n",
    "\n",
    "```\n",
    "Click here to view code image\n",
    "   x, y = 0, 5\n",
    "   result = divide(x, y)\n",
    "   if not result:\n",
    "       print(‘Invalid inputs’)  # This is wrong!\n",
    "       \n",
    "```\n",
    "This is a common mistake in Python code when None has special meaning. This is why returning None from a function is error prone. There are two ways to reduce the chance of such errors.\n",
    "The first way is to split the return value into a two-tuple. The first part of the tuple indicates that the operation was a success or failure. The second part is the actual result that was computed.\n",
    "￼￼￼￼\n",
    "```\n",
    "def divide(a, b):\n",
    "       try:\n",
    "           return True, a / b\n",
    "       except ZeroDivisionError:\n",
    "           return False, None\n",
    "           \n",
    "```\n",
    "Callers of this function have to unpack the tuple. That forces them to consider the status part of the tuple instead of just looking at the result of division.\n",
    "```\n",
    "Click here to view code image\n",
    "   success, result = divide(x, y)\n",
    "   if not success:\n",
    "       print(‘Invalid inputs’)\n",
    "       \n",
    "```\n",
    "The problem is that callers can easily ignore the first part of the tuple (using the underscore variable name, a Python convention for unused variables). The resulting code doesn’t look wrong at first glance. This is as bad as just returning None.\n",
    "```\n",
    "   _, result = divide(x, y)\n",
    "   if not result:\n",
    "       print(‘Invalid inputs’)\n",
    "       \n",
    "```\n",
    "The second, better way to reduce these errors is to never return None at all. Instead, raise an exception up to the caller and make them deal with it. Here, I turn a ZeroDivisionError into a ValueError to indicate to the caller that the input values are bad:\n",
    "\n",
    "```\n",
    "Click here to view code image\n",
    "   def divide(a, b):\n",
    "       try:\n",
    "           return a / b\n",
    "       except ZeroDivisionError as e:\n",
    "           raise ValueError(‘Invalid inputs’) from e\n",
    "```\n",
    "Now the caller should handle the exception for the invalid input case (this behavior should be documented; see Item 49: “Write Docstrings for Every Function, Class, and Module”). The caller no longer requires a condition on the return value of the function. If the function didn’t raise an exception, then the return value must be good. The outcome of exception handling is clear.\n",
    "```\n",
    "Click here to view code image\n",
    "x, y = 5, 2 try:\n",
    "       result = divide(x, y)\n",
    "   except ValueError:\n",
    "       print(‘Invalid inputs’)\n",
    "   else:\n",
    "       print(‘Result is %.1f’ % result)\n",
    "   >>>\n",
    "Result is 2.5\n",
    "\n",
    "```\n",
    "Things to Remember\n",
    "Functions that return None to indicate special meaning are error prone because None and other values (e.g., zero, the empty string) all evaluate to False in conditional expressions.\n",
    "￼￼￼￼￼￼\n",
    "Raise exceptions to indicate special situations instead of returning None. Expect the calling code to handle exceptions properly when they’re documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 15: Know How Closures Interact with Variable Scope\n",
    "\n",
    "Say you want to sort a list of numbers but prioritize one group of numbers to come first. This pattern is useful when you’re rendering a user interface and want important messages or exceptional events to be displayed before everything else.\n",
    "\n",
    "A common way to do this is to pass a helper function as the key argument to a list’s sort method. The helper’s return value will be used as the value for sorting each item in the list. The helper can check whether the given item is in the important group and can vary the sort key accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "def sort_priority(values, group):\n",
    "       def helper(x):\n",
    "           if x in group:\n",
    "               return (0, x)\n",
    "           return (1, x)\n",
    "       values.sort(key=helper)\n",
    "\n",
    "numbers = [8, 3, 1, 2, 5, 4, 7, 6]\n",
    "group = {2, 3, 5, 7}\n",
    "sort_priority(numbers, group)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three reasons why this function operates as expected:\n",
    "* Python supports closures: functions that refer to variables from the scope in which they were defined. This is why the helper function is able to access the group argument to sort_priority.\n",
    "* Functions are first-class objects in Python, meaning you can refer to them directly, assign them to variables, pass them as arguments to other functions, compare them in expressions and if statements, etc. This is how the sort method can accept a closure function as the key argument.\n",
    "** * Python has specific rules for comparing tuples. It first compares items in index zero, then index one, then index two, and so on. This is why the return value from the helper closure causes the sort order to have two distinct groups.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’d be nice if this function returned whether higher-priority items were seen at all so the user interface code can act accordingly. Adding such behavior seems straightforward. There’s already a closure function for deciding which group each number is in. Why not also use the closure to flip a flag when high-priority items are seen? Then the function can return the flag value after it’s been modified by the closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_priority2(numbers, group):\n",
    "       found = False\n",
    "       def helper(x):\n",
    "           if x in group:\n",
    "               found = True  # Seems simple\n",
    "               return (0, x)\n",
    "           return (1, x)\n",
    "       numbers.sort(key=helper)\n",
    "       return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: False\n",
      "[2, 3, 5, 7, 1, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "found = sort_priority2(numbers, group)\n",
    "print('Found:', found)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sorted results are correct, but the found result is wrong. Items from group were definitely found in numbers, but the function returned False. How could this happen?\n",
    "When you reference a variable in an expression, the Python interpreter will traverse the scope to resolve the reference in this order:\n",
    "LEGB Rule\n",
    "=========\n",
    "1. The current function’s scope\n",
    "2. Any enclosing scopes (like other containing functions)\n",
    "3. The scope of the module that contains the code (also called the global scope)\n",
    "4. The built-in scope (that contains functions like len and str)\n",
    "\n",
    "\n",
    "If none of these places have a defined variable with the referenced name, then a NameError exception is raised.\n",
    "\n",
    "Assigning a value to a variable works differently. If the variable is already defined in the current scope, then it will just take on the new value. If the variable doesn’t exist in the current scope, then Python treats the assignment as a variable definition. The scope of the newly defined variable is the function that contains the assignment.\n",
    "This assignment behavior explains the wrong return value of the sort_priority2 function. The found variable is assigned to True in the helper closure. The closure’s assignment is treated as a new variable definition within helper, not as an assignment within sort_priority2.\n",
    "\n",
    "Encountering this problem is sometimes called the scoping bug because it can be so surprising to newbies. But this is the intended result. This behavior prevents local variables in a function from polluting the containing module. Otherwise, every assignment within a function would put garbage into the global module scope. Not only would that be noise, but the interplay of the resulting global variables could cause obscure bugs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting Data Out\n",
    "In Python 3, there is special syntax for getting data out of a closure. The nonlocal statement is used to indicate that scope traversal should happen upon assignment for a specific variable name. The only limit is that nonlocal won’t traverse up to the module- level scope (to avoid polluting globals).\n",
    "Here, I define the same function again using nonlocal:\n",
    "```\n",
    "Click here to view code image\n",
    "   def sort_priority3(numbers, group):\n",
    "       found = False\n",
    "       def helper(x):\n",
    "           nonlocal found\n",
    "           if x in group:\n",
    "               found = True\n",
    "               return (0, x)\n",
    "           return (1, x)\n",
    "       numbers.sort(key=helper)\n",
    "       return found\n",
    "```\n",
    " \n",
    "The nonlocal statement makes it clear when data is being assigned out of a closure into another scope. It’s complementary to the global statement, which indicates that a variable’s assignment should go directly into the module scope.\n",
    "However, much like the anti-pattern of global variables, I’d caution against using nonlocal for anything beyond simple functions. The side effects of nonlocal can be hard to follow. It’s especially hard to understand in long functions where the nonlocal statements and assignments to associated variables are far apart.\n",
    "When your usage of nonlocal starts getting complicated, it’s better to wrap your state in a helper class. Here, I define a class that achieves the same result as the nonlocal approach. It’s a little longer, but is much easier to read (see Item 23: “Accept Functions for Simple Interfaces Instead of Classes” for details on the __call__ special method).\n",
    "```Click here to view code image\n",
    "   class Sorter(object):\n",
    "       def __init__(self, group):\n",
    "           self.group = group\n",
    "           self.found = False\n",
    "       def __call__(self, x):\n",
    "           if x in self.group:\n",
    "￼￼￼￼￼self.found = True\n",
    "return (0, x)\n",
    "           return (1, x)\n",
    "   sorter = Sorter(group)\n",
    "   numbers.sort(key=sorter)\n",
    "   assert sorter.found is True\n",
    "```\n",
    "\n",
    "##### Scope in Python 2\n",
    "Unfortunately, Python 2 doesn’t support the nonlocal keyword. In order to get similar behavior, you need to use a work-around that takes advantage of Python’s scoping rules. This approach isn’t pretty, but it’s the common Python idiom.\n",
    "``` Click here to view code image\n",
    "# Python 2\n",
    "   def sort_priority(numbers, group):\n",
    "       found = [False]\n",
    "       def helper(x):\n",
    "           if x in group:\n",
    "               found[0] = True\n",
    "               return (0, x)\n",
    "           return (1, x)\n",
    "       numbers.sort(key=helper)\n",
    "       return found[0]\n",
    "```\n",
    "As explained above, Python will traverse up the scope where the found variable is referenced to resolve its current value. The trick is that the value for found is a list, which is mutable. This means that once retrieved, the closure can modify the state of found to send data out of the inner scope (with found[0] = True).\n",
    "This approach also works when the variable used to traverse the scope is a dictionary, a set, or an instance of a class you’ve defined.\n",
    "\n",
    "Things to Remember\n",
    "* Closure functions can refer to variables from any of the scopes in which they were defined.\n",
    "* By default, closures can’t affect enclosing scopes by assigning variables.\n",
    "* In Python 3, use the nonlocal statement to indicate when a closure can modify a variable in its enclosing scopes.\n",
    "* In Python 2, use a mutable value (like a single-item list) to work around the lack of the nonlocal statement.\n",
    "* Avoid using nonlocal statements for anything beyond simple functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 16: Consider Generators Instead of Returning Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest choice for functions that produce a sequence of results is to return a list of items. For example, say you want to find the index of every word in a string. Here, I accumulate results in a list using the append method and return it at the end of the function:\n",
    "￼￼￼￼￼￼\n",
    "```\n",
    "Click here to view code image\n",
    "   def index_words(text):\n",
    "       result = []\n",
    "       if text:\n",
    "           result.append(0)\n",
    "       for index, letter in enumerate(text):\n",
    "           if letter == ‘ ‘:\n",
    "               result.append(index + 1)\n",
    "       return result\n",
    "```\n",
    "This works as expected for some sample input.\n",
    "\n",
    "``` \n",
    "\n",
    "Click here to view code image\n",
    "   address = ‘Four score and seven years ago...’\n",
    "   result = index_words(address)\n",
    "   print(result[:3])\n",
    "   \n",
    "    [0, 5, 11]\n",
    "\n",
    "```\n",
    "There are two problems with the index_words function.\n",
    "The first problem is that the code is a bit dense and noisy. Each time a new result is found, I call the append method. The method call’s bulk (result.append) deemphasizes the value being added to the list (index + 1). There is one line for creating the result list and another for returning it. While the function body contains ~130 characters (without whitespace), only ~75 characters are important.\n",
    "A better way to write this function is using a generator. Generators are functions that use yield expressions. When called, generator functions do not actually run but instead immediately return an iterator. With each call to the next built-in function, the iterator will advance the generator to its next yield expression. Each value passed to yield by the generator will be returned by the iterator to the caller.\n",
    "Here, I define a generator function that produces the same results as before:\n",
    "```Click here to view code image\n",
    "   def index_words_iter(text):\n",
    "       if text:\n",
    "           yield 0\n",
    "       for index, letter in enumerate(text):\n",
    "           if letter == ‘ ‘:\n",
    "               yield index + 1\n",
    "```\n",
    "It’s significantly easier to read because all interactions with the result list have been eliminated. Results are passed to yield expressions instead. The iterator returned by the generator call can easily be converted to a list by passing it to the list built-in function (see Item 9: “Consider Generator Expressions for Large Comprehensions” for how this works).\n",
    "```Click here to view code image\n",
    "   result = list(index_words_iter(address))\n",
    "```\n",
    "The second problem with index_words is that it requires all results to be stored in the list before being returned. For huge inputs, this can cause your program to run out of\n",
    "￼￼￼￼￼￼\n",
    "memory and crash. In contrast, a generator version of this function can easily be adapted to take inputs of arbitrary length.\n",
    "Here, I define a generator that streams input from a file one line at a time and yields outputs one word at a time. The working memory for this function is bounded to the maximum length of one line of input.\n",
    "```  \n",
    "    \n",
    "    def index_file(handle):\n",
    "       offset = 0\n",
    "       for line in handle:\n",
    "           if line:\n",
    "               yield offset\n",
    "           for letter in line:\n",
    "               offset += 1\n",
    "               if letter == ‘ ‘:\n",
    "yield offset\n",
    "\n",
    "```\n",
    "\n",
    "Running the generator produces the same results.\n",
    "``` \n",
    "\n",
    "Click here to view code image\n",
    "   with open(‘/tmp/address.txt’, ‘r’) as f:\n",
    "       it = index_file(f)\n",
    "       results = islice(it, 0, 3)\n",
    "       print(list(results))\n",
    ">>>\n",
    "[0, 5, 11]\n",
    "\n",
    "```\n",
    "The only gotcha of defining generators like this is that the callers must be aware that the iterators returned are stateful and can’t be reused (see Item 17: “Be Defensive When Iterating Over Arguments”).\n",
    "Things to Remember\n",
    "Using generators can be clearer than the alternative of returning lists of accumulated results.\n",
    "The iterator returned by a generator produces the set of values passed to yield expressions within the generator function’s body.\n",
    "Generators can produce a sequence of outputs for arbitrarily large inputs because their working memory doesn’t include all inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 17: Be Defensive When Iterating Over Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a function takes a list of objects as a parameter, it’s often important to iterate over that list multiple times. For example, say you want to analyze tourism numbers for the U.S. state of Texas. Imagine the data set is the number of visitors to each city (in millions per year). You’d like to figure out what percentage of overall tourism each city receives.\n",
    "To do this you need a normalization function. It sums the inputs to determine the total number of tourists per year. Then it divides each city’s individual visitor count by the total to find that city’s contribution to the whole.\n",
    "``` \n",
    "Click here to view code image\n",
    "￼￼￼￼￼￼￼￼\n",
    "   def normalize(numbers):\n",
    "       total = sum(numbers)\n",
    "       result = []\n",
    "       for value in numbers:\n",
    "           percent = 100 * value / total\n",
    "           result.append(percent)\n",
    "       return result\n",
    "```\n",
    "\n",
    "This function works when given a list of visits.\n",
    "```\n",
    "Click here to view code image\n",
    "   visits = [15, 35, 80]\n",
    "   percentages = normalize(visits)\n",
    "   print(percentages)\n",
    "   >>>\n",
    "   [11.538461538461538, 26.923076923076923, 61.53846153846154]\n",
    "   \n",
    "```\n",
    "To scale this up, I need to read the data from a file that contains every city in all of Texas. I define a generator to do this because then I can reuse the same function later when I want to compute tourism numbers for the whole world, a much larger data set (see Item 16: “Consider Generators Instead of Returning Lists”).\n",
    "```\n",
    "Click here to view code image\n",
    "   def read_visits(data_path):\n",
    "       with open(data_path) as f:\n",
    "           for line in f:\n",
    "               yield int(line)\n",
    "Surprisingly, calling normalize on the generator’s return value produces no results.\n",
    "Click here to view code image\n",
    "   it = read_visits(‘/tmp/my_numbers.txt’)\n",
    "   percentages = normalize(it)\n",
    "   print(percentages)\n",
    ">>> []\n",
    "\n",
    "```\n",
    "\n",
    "The cause of this behavior is that an iterator only produces its results a single time. If you iterate over an iterator or generator that has already raised a StopIteration exception, you won’t get any results the second time around.\n",
    "```\n",
    "Click here to view code image\n",
    "   it = read_visits(‘/tmp/my_numbers.txt’)\n",
    "   print(list(it))\n",
    "   print(list(it))  # Already exhausted\n",
    "   >>>\n",
    "   [15, 35, 80]\n",
    "   []\n",
    "   \n",
    "```\n",
    "What’s confusing is that you also won’t get any errors when you iterate over an already exhausted iterator. for loops, the list constructor, and many other functions throughout the Python standard library expect the StopIteration exception to be raised during normal operation. These functions can’t tell the difference between an iterator that has no output and an iterator that had output and is now exhausted.\n",
    "￼￼￼￼￼￼\n",
    "To solve this problem, you can explicitly exhaust an input iterator and keep a copy of its entire contents in a list. You can then iterate over the list version of the data as many times as you need to. Here’s the same function as before, but it defensively copies the input iterator:\n",
    "```\n",
    "Click here to view code image\n",
    "   def normalize_copy(numbers):\n",
    "       numbers = list(numbers)  # Copy the iterator\n",
    "       total = sum(numbers)\n",
    "       result = []\n",
    "       for value in numbers:\n",
    "           percent = 100 * value / total\n",
    "           result.append(percent)\n",
    "       return result\n",
    "       \n",
    "```\n",
    "Now the function works correctly on a generator’s return value.\n",
    "```\n",
    "Click here to view code image\n",
    "   it = read_visits(‘/tmp/my_numbers.txt’)\n",
    "   percentages = normalize_copy(it)\n",
    "   print(percentages)\n",
    "   >>>\n",
    "   [11.538461538461538, 26.923076923076923, 61.53846153846154]\n",
    "```\n",
    "\n",
    "The problem with this approach is the copy of the input iterator’s contents could be large. Copying the iterator could cause your program to run out of memory and crash. One way around this is to accept a function that returns a new iterator each time it’s called.\n",
    "```\n",
    "Click here to view code image\n",
    "   def normalize_func(get_iter):\n",
    "       total = sum(get_iter())   # New iterator\n",
    "       result = []\n",
    "       for value in get_iter():  # New iterator\n",
    "           percent = 100 * value / total\n",
    "           result.append(percent)\n",
    "       return result\n",
    "```\n",
    "To use normalize_func, you can pass in a lambda expression that calls the generator and produces a new iterator each time.\n",
    "```\n",
    "Click here to view code image\n",
    "   percentages = normalize_func(lambda: read_visits(path))\n",
    "```\n",
    "\n",
    "Though it works, having to pass a lambda function like this is clumsy. The better way to achieve the same result is to provide a new container class that implements the iterator protocol.\n",
    "The iterator protocol is how Python for loops and related expressions traverse the contents of a container type. When Python sees a statement like for x in foo it will actually call iter(foo). The iter built-in function calls the foo.__iter__ special method in turn. The __iter__ method must return an iterator object (which itself implements the __next__ special method). Then the for loop repeatedly calls the next built-in function on the iterator object until it’s exhausted (and raises a StopIteration exception).\n",
    "￼￼￼￼\n",
    "It sounds complicated, but practically speaking you can achieve all of this behavior for your classes by implementing the __iter__ method as a generator. Here, I define an iterable container class that reads the files containing tourism data:\n",
    "```\n",
    "Click here to view code image\n",
    "   class ReadVisits(object):\n",
    "       def __init__(self, data_path):\n",
    "           self.data_path = data_path\n",
    "       def __iter__(self):\n",
    "           with open(self.data_path) as f:\n",
    "               for line in f:\n",
    "                   yield int(line)\n",
    "```\n",
    "\n",
    "This new container type works correctly when passed to the original function without any modifications.\n",
    "```\n",
    "Click here to view code image\n",
    "   visits = ReadVisits(path)\n",
    "   percentages = normalize(visits)\n",
    "   print(percentages)\n",
    "   >>>\n",
    "   [11.538461538461538, 26.923076923076923, 61.53846153846154]\n",
    "```\n",
    "\n",
    "This works because the sum method in normalize will call ReadVisits.__iter__ to allocate a new iterator object. The for loop to normalize the numbers will also call __iter__ to allocate a second iterator object. Each of those iterators will be advanced and exhausted independently, ensuring that each unique iteration sees all of the input data values. The only downside of this approach is that it reads the input data multiple times.\n",
    "Now that you know how containers like ReadVisits work, you can write your functions to ensure that parameters aren’t just iterators. The protocol states that when an iterator is passed to the iter built-in function, iter will return the iterator itself. In contrast, when a container type is passed to iter, a new iterator object will be returned each time. Thus, you can test an input value for this behavior and raise a TypeError to reject iterators.\n",
    "\n",
    "```Click here to view code image\n",
    "   def normalize_defensive(numbers):\n",
    "       if iter(numbers) is iter(numbers):  # An iterator — bad!\n",
    "           raise TypeError(‘Must supply a container’)\n",
    "       total = sum(numbers)\n",
    "       result = []\n",
    "       for value in numbers:\n",
    "           percent = 100 * value / total\n",
    "           result.append(percent)\n",
    "       return result\n",
    "```\n",
    "This is ideal if you don’t want to copy the full input iterator like normalize_copy above, but you also need to iterate over the input data multiple times. This function works as expected for list and ReadVisits inputs because they are containers. It will work for any type of container that follows the iterator protocol.\n",
    "￼￼￼\n",
    "``` here to view code image\n",
    "   visits = [15, 35, 80]\n",
    "   normalize_defensive(visits)  # No error\n",
    "   visits = ReadVisits(path)\n",
    "   normalize_defensive(visits)  # No error\n",
    "```\n",
    "\n",
    "The function will raise an exception if the input is iterable but not a container.\n",
    "``` \n",
    "\n",
    "Click here to view code image\n",
    "   it = iter(visits)\n",
    "   normalize_defensive(it)\n",
    "   >>>\n",
    "   TypeError: Must supply a container\n",
    "```\n",
    "Things to Remember\n",
    "Beware of functions that iterate over input arguments multiple times. If these arguments are iterators, you may see strange behavior and missing values.\n",
    "Python’s iterator protocol defines how containers and iterators interact with the iter and next built-in functions, for loops, and related expressions.\n",
    "You can easily define your own iterable container type by implementing the __iter__ method as a generator.\n",
    "You can detect that a value is an iterator (instead of a container) if calling iter on it twice produces the same result, which can then be progressed with the next built- in function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 18: Reduce Visual Noise with Variable Positional Arguments\n",
    "\n",
    "Accepting optional positional arguments (often called star args in reference to the conventional name for the parameter, *args) can make a function call more clear and remove visual noise.\n",
    "For example, say you want to log some debug information. With a fixed number of arguments, you would need a function that takes a message and a list of values.\n",
    "```\n",
    "Click here to view code image\n",
    "   def log(message, values):\n",
    "       if not values:\n",
    "           print(message)\n",
    "       else:\n",
    "           values_str = ‘, ‘.join(str(x) for x in values)\n",
    "           print(‘%s: %s’ % (message, values_str))\n",
    "   log(‘My numbers are’, [1, 2])\n",
    "   log(‘Hi there’, [])\n",
    "   >>>\n",
    "   My numbers are: 1, 2\n",
    "   Hi there\n",
    "￼￼￼￼￼￼￼\n",
    "```\n",
    "\n",
    "Having to pass an empty list when you have no values to log is cumbersome and noisy. It’d be better to leave out the second argument entirely. You can do this in Python by prefixing the last positional parameter name with *. The first parameter for the log message is required, whereas any number of subsequent positional arguments are optional. The function body doesn’t need to change, only the callers do.\n",
    "```\n",
    "Click here to view code image\n",
    "   def log(message, *values):  # The only difference\n",
    "       if not values:\n",
    "           print(message)\n",
    "       else:\n",
    "           values_str = ‘, ‘.join(str(x) for x in values)\n",
    "           print(‘%s: %s’ % (message, values_str))\n",
    "   log(‘My numbers are’, 1, 2)\n",
    "   log(‘Hi there’)  # Much better\n",
    "   >>>\n",
    "   My numbers are: 1, 2\n",
    "   Hi there\n",
    "```\n",
    "If you already have a list and want to call a variable argument function like log, you can do this by using the * operator. This instructs Python to pass items from the sequence as positional arguments.\n",
    "```\n",
    "Click here to view code image\n",
    "   favorites = [7, 33, 99]\n",
    "   log(‘Favorite colors’, *favorites)\n",
    "   >>>\n",
    "   Favorite colors: 7, 33, 99\n",
    "```\n",
    "\n",
    "There are two problems with accepting a variable number of positional arguments.\n",
    "The first issue is that the variable arguments are always turned into a tuple before they are passed to your function. This means that if the caller of your function uses the * operator on a generator, it will be iterated until it’s exhausted. The resulting tuple will include every value from the generator, which could consume a lot of memory and cause your program to crash.\n",
    "\n",
    "```\n",
    "Click here to view code image\n",
    "   def my_generator():\n",
    "       for i in range(10):\n",
    "           yield i\n",
    "   def my_func(*args):\n",
    "       print(args)\n",
    "   it = my_generator()\n",
    "   my_func(*it)\n",
    "   >>>\n",
    "   (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n",
    "```\n",
    "\n",
    "Functions that accept *args are best for situations where you know the number of inputs in the argument list will be reasonably small. It’s ideal for function calls that pass many\n",
    "￼￼￼\n",
    "literals or variable names together. It’s primarily for the convenience of the programmer and the readability of the code.\n",
    "The second issue with *args is that you can’t add new positional arguments to your function in the future without migrating every caller. If you try to add a positional argument in the front of the argument list, existing callers will subtly break if they aren’t updated.\n",
    "```\n",
    "Click here to view code image\n",
    "   def log(sequence, message, *values):\n",
    "       if not values:\n",
    "           print(‘%s: %s’ % (sequence, message))\n",
    "       else:\n",
    "           values_str = ‘, ‘.join(str(x) for x in values)\n",
    "           print(‘%s: %s: %s’ % (sequence, message, values_str))\n",
    "   log(1, ‘Favorites’, 7, 33)      # New usage is OK\n",
    "   log(‘Favorite numbers’, 7, 33)  # Old usage breaks\n",
    "   >>>\n",
    "   1: Favorites: 7, 33\n",
    "   Favorite numbers: 7: 33\n",
    "```\n",
    "The problem here is that the second call to log used 7 as the message parameter because a sequence argument wasn’t given. Bugs like this are hard to track down because the code still runs without raising any exceptions. To avoid this possibility entirely, you should use keyword-only arguments when you want to extend functions that accept *args (see Item 21: “Enforce Clarity with Keyword-Only Arguments”).\n",
    "Things to Remember\n",
    "Functions can accept a variable number of positional arguments by using *args in the def statement.\n",
    "You can use the items from a sequence as the positional arguments for a function with the * operator.\n",
    "Using the * operator with a generator may cause your program to run out of memory and crash.\n",
    "Adding new positional parameters to functions that accept *args can introduce hard-to-find bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 19: Provide Optional Behavior with Keyword Arguments\n",
    "\n",
    "Like most other programming languages, calling a function in Python allows for passing arguments by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remainder(number, divisor):\n",
    "       return number % divisor\n",
    "remainder(20, 3)\n",
    "remainder(20, divisor=3)\n",
    "remainder(number=20, divisor=3)\n",
    "remainder(divisor=3 , number=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All positional arguments to Python functions can also be passed by keyword, where the name of the argument is used in an assignment within the parentheses of a function call. The keyword arguments can be passed in any order as long as all of the required positional arguments are specified. You can mix and match keyword and positional arguments. These calls are equivalent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Function arguments can be specified by position or by keyword.\n",
    "* Keywords make it clear what the purpose of each argument is when it would be confusing with only positional arguments.\n",
    "* Keyword arguments with default values make it easy to add new behaviors to a function, especially when the function has existing callers.\n",
    "* Optional keyword arguments should always be passed by keyword instead of by position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 20: Use None and Docstrings to Specify Dynamic Default Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you need to use a non-static type as a keyword argument’s default value. For example, say you want to print logging messages that are marked with the time of the logged event. In the default case, you want the message to include the time when the function was called. You might try the following approach, assuming the default arguments are reevaluated each time the function is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-26 20:10:34.937349: Hello\n",
      "2016-03-26 20:10:34.937349: World\n",
      "2016-03-26 20:10:34.937349: World2\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def log(message, when=dt.datetime.now()):\n",
    "       print('%s: %s' % (when, message))\n",
    "        \n",
    "log(\"Hello\")\n",
    "log(\"World\")\n",
    "log(\"World2\")   \n",
    "# all have same timestamp which is woring...reason ebing is that when the function object is created at\n",
    "# load time dattime is evaluated and kep in defaultkeys hence you have same when."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convention for achieving the desired result in Python is to provide a default value of None and to document the actual behavior in the docstring (see Item 49: “Write Docstrings for Every Function, Class, and Module”). When your code sees an argument value of None, you allocate the default value accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-03-26 20:13:04.918648: Hello\n",
      "2016-03-26 20:13:04.918748: World\n",
      "2016-03-26 20:13:04.918798: World2\n"
     ]
    }
   ],
   "source": [
    "def log(message, when=None):\n",
    "       \"\"\" Log a message with a timestamp.\n",
    "       Args:\n",
    "           message: Message to print.\n",
    "           when: datetime of when the message occurred.\n",
    "               Defaults to the present time.\n",
    "       \"\"\"\n",
    "       when = dt.datetime.now() if when is None else when\n",
    "       print('%s: %s' % (when, message))\n",
    "        \n",
    "log(\"Hello\")\n",
    "log(\"World\")\n",
    "log(\"World2\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using None for default argument values is especially important when the arguments are mutable. For example, say you want to load a value encoded as JSON data. If decoding the data fails, you want an empty dictionary to be returned by default. You might try this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo: {'meep': 1, 'stuff': 5}\n",
      "Bar: {'meep': 1, 'stuff': 5}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def decode(data, default={}):\n",
    "       try:\n",
    "           return json.loads(data)\n",
    "       except ValueError:\n",
    "           return default\n",
    "\n",
    "foo = decode('bad data')\n",
    "foo['stuff'] = 5\n",
    "bar = decode('also bad')\n",
    "bar['meep'] = 1\n",
    "print('Foo:', foo)\n",
    "print('Bar:', bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is the same as the datetime.now example above. The dictionary specified for default will be shared by all calls to decode because default argument values are only evaluated once (at module load time). This can cause extremely surprising behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You’d expect two different dictionaries, each with a single key and value. But modifying one seems to also modify the other. The culprit is that foo and bar are both equal to the default parameter. They are the same dictionary object.\n",
    "assert foo is bar\n",
    "The fix is to set the keyword argument default value to None and then document the\n",
    "behavior in the function’s docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo: {'stuff': 5}\n",
      "Bar: {'meep': 1}\n"
     ]
    }
   ],
   "source": [
    "def decode(data, default=None):\n",
    "       \"\"\"\n",
    "       Load JSON data from a string.\n",
    "       Args:\n",
    "           data: JSON data to decode.\n",
    "           default: Value to return if decoding fails.\n",
    "               Defaults to an empty dictionary.\n",
    "       \"\"\"\n",
    "       if default is None:\n",
    "           default = {}\n",
    "       try:\n",
    "           return json.loads(data)\n",
    "       except ValueError:\n",
    "           return default\n",
    "        \n",
    "foo = decode('bad data')\n",
    "foo['stuff'] = 5\n",
    "bar = decode('also bad')\n",
    "bar['meep'] = 1\n",
    "print('Foo:', foo)\n",
    "print('Bar:', bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Default arguments are only evaluated once: during function definition at module load time. This can cause odd behaviors for dynamic values (like {} or []).\n",
    "* Use None as the default value for keyword arguments that have a dynamic value. Document the actual default behavior in the function’s docstring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 21: Enforce Clarity with Keyword-Only Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def safe_division(number, divisor, ignore_overflow=True,\n",
    "                     ignore_zero_division=False):\n",
    "       try:\n",
    "           return number / divisor\n",
    "       except OverflowError:\n",
    "           if ignore_overflow:\n",
    "               return 0\n",
    "           else:\n",
    "               raise\n",
    "       except ZeroDivisionError:\n",
    "           if ignore_zero_division:\n",
    "               return float('inf')\n",
    "           else: \n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "result = safe_division(1, 10**500, True, False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_division(1, 10**500, ignore_overflow=True)\n",
    "safe_division(1, 0, ignore_zero_division=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Keyword arguments make the intention of a function call more clear.\n",
    "* Use keyword-only arguments to force callers to supply keyword arguments for potentially confusing functions, especially those that accept multiple Boolean flags.\n",
    "* Python 3 supports explicit syntax for keyword-only arguments in functions.\n",
    "* Python 2 can emulate keyword-only arguments for functions by using **kwargs and manually raising TypeError exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and Inheritance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an object-oriented programming language, Python supports a full range of features, such as inheritance, polymorphism, and encapsulation. Getting things done in Python often requires writing new classes and defining how they interact through their interfaces and hierarchies.\n",
    "\n",
    "Python’s classes and inheritance make it easy to express your program’s intended behaviors with objects. They allow you to improve and expand functionality over time. They provide flexibility in an environment of changing requirements. Knowing how to use them well enables you to write maintainable code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 22: Prefer Helper Classes Over Bookkeeping with Dictionaries and Tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Refactoring to Classes\n",
    "You can start moving to classes at the bottom of the dependency tree: a single grade. A class seems too heavyweight for such simple information. A tuple, though, seems appropriate because grades are immutable. Here, I use the tuple (score, weight) to track grades in a list:\n",
    "```\n",
    "Click here to view code image\n",
    "grades = []\n",
    "grades.append((95, 0.45))\n",
    "#...\n",
    "total = sum(score * weight for score, weight in grades) total_weight = sum(weight for _, weight in grades) average_grade = total / total_weight\n",
    "```\n",
    "The problem is that plain tuples are positional. When you want to associate more information with a grade, like a set of notes from the teacher, you’ll need to rewrite every usage of the two-tuple to be aware that there are now three items present instead of two. Here, I use _ (the underscore variable name, a Python convention for unused variables) to capture the third entry in the tuple and just ignore it:\n",
    "```\n",
    "Click here to view code image\n",
    "grades = []\n",
    "grades.append((95, 0.45, ‘Great job’))\n",
    "#...\n",
    "total = sum(score * weight for score, weight, _ in grades)\n",
    "￼￼￼\n",
    "total_weight = sum(weight for _, weight, _ in grades)\n",
    "   average_grade = total / total_weight\n",
    "```\n",
    "\n",
    "This pattern of extending tuples longer and longer is similar to deepening layers of dictionaries. As soon as you find yourself going longer than a two-tuple, it’s time to consider another approach.\n",
    "The namedtuple type in the collections module does exactly what you need. It lets you easily define tiny, immutable data classes.\n",
    "```\n",
    "Click here to view code image\n",
    "   import collections\n",
    "   Grade = collections.namedtuple(‘Grade’, (‘score’, ‘weight’))\n",
    "```\n",
    "These classes can be constructed with positional or keyword arguments. The fields are accessible with named attributes. Having named attributes makes it easy to move from a namedtuple to your own class later if your requirements change again and you need to add behaviors to the simple data containers.\n",
    "\n",
    "\n",
    "#### Limitations of namedtuple\n",
    "* Although useful in many circumstances, it’s important to understand when\n",
    "namedtuple can cause more harm than good.\n",
    "\n",
    "* You can’t specify default argument values for namedtuple classes. This makes them unwieldy when your data may have many optional properties. If you find yourself using more than a handful of attributes, defining your own class may be a better choice.\n",
    "\n",
    "* The attribute values of namedtuple instances are still accessible using numerical indexes and iteration. Especially in externalized APIs, this can lead to unintentional usage that makes it harder to move to a real class later. If you’re not in control of all of the usage of your namedtuple instances, it’s better to define your own class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avoid making dictionaries with values that are other dictionaries or long tuples.\n",
    "* Use namedtuple for lightweight, immutable data containers before you need the flexibility of a full class.\n",
    "* Move your bookkeeping code to use multiple helper classes when your internal state dictionaries get complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 23: Accept Functions for Simple Interfaces Instead of Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of Python’s built-in APIs allow you to customize behavior by passing in a function. These hooks are used by APIs to call back your code while they execute. For example, the list type’s sort method takes an optional key argument that’s used to determine each index’s value for sorting. Here, I sort a list of names based on their lengths by providing a lambda expression as the key hook:\n",
    "\n",
    "In other languages, you might expect hooks to be defined by an abstract class. In Python, many hooks are just stateless functions with well-defined arguments and return values. Functions are ideal for hooks because they are easier to describe and simpler to define than classes. Functions work as hooks because Python has first-class functions: Functions and methods can be passed around and referenced like any other value in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plato', 'Socrates', 'Aristotle', 'Archimedes']\n"
     ]
    }
   ],
   "source": [
    "names = ['Socrates', 'Archimedes', 'Plato', 'Aristotle']\n",
    "names.sort(key=lambda x: len(x))\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of defining and instantiating classes, functions are often all you need for simple interfaces between components in Python.\n",
    "* References to functions and methods in Python are first class, meaning they can be\n",
    "￼￼￼￼\n",
    "* used in expressions like any other type.\n",
    "* The __call__ special method enables instances of a class to be called like plain Python functions.\n",
    "* When you need a function to maintain state, consider defining a class that provides the __call__ method instead of defining a stateful closure (see Item 15: “Know How Closures Interact with Variable Scope”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 24: Use @classmethod Polymorphism to ConstructObjects Generically\n",
    "\n",
    "** In Python, not only do the objects support polymorphism, but the classes do as well. What does that mean, and what is it good for?**\n",
    "\n",
    "Polymorphism is a way for multiple classes in a hierarchy to implement their own unique versions of a method. This allows many classes to fulfill the same interface or abstract base class while providing different functionality (see Item 28: “Inherit from collections.abc for Custom Container Types” for an example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Python only supports a single constructor per class, the __init__ method. \n",
    "* Use @classmethod to define alternative constructors for your classes.\n",
    "* Use class method polymorphism to provide generic ways to build and connect concrete subclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 25: Initialize Parent Classes with super\n",
    "\n",
    "The old way to initialize a parent class from a child class is to directly call the parent\n",
    "class’s __init__ method with the child instance.\n",
    "\n",
    "```\n",
    "class MyBaseClass(object):\n",
    "       def __init__(self, value):\n",
    "            self.value = value\n",
    "            \n",
    "class MyChildClass(MyBaseClass):\n",
    "       def __init__(self):\n",
    "            MyBaseClass.__init__(self, 5)\n",
    "```\n",
    "This approach works fine for simple hierarchies but breaks down in many cases.\n",
    "\n",
    "This approach works fine for simple hierarchies but breaks down in many cases.\n",
    "If your class is affected by multiple inheritance (something to avoid in general; see Item 26: “Use Multiple Inheritance Only for Mix-in Utility Classes”), calling the superclasses’ __init__ methods directly can lead to unpredictable behavior.\n",
    "One problem is that the __init__ call order isn’t specified across all subclasses. For example, here I define two parent classes that operate on the instance’s value field:\n",
    "\n",
    "```   \n",
    "\n",
    " ```\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ordering is (5 * 2) + 5 = 15\n"
     ]
    }
   ],
   "source": [
    "class MyBaseClass(object):\n",
    "       def __init__(self, value):\n",
    "           self.value = value\n",
    "class TimesTwo(object):\n",
    "    def __init__(self):\n",
    "           self.value *= 2\n",
    "class PlusFive(object):\n",
    "    def __init__(self):\n",
    "           self.value += 5\n",
    "            \n",
    "class OneWay(MyBaseClass, TimesTwo, PlusFive):\n",
    "       def __init__(self, value):\n",
    "           MyBaseClass.__init__(self, value)\n",
    "           TimesTwo.__init__(self)\n",
    "           PlusFive.__init__(self)           \n",
    "\n",
    "foo = OneWay(5)\n",
    "print('First ordering is (5 * 2) + 5 =', foo.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The old way to initialize a parent class from a child class is to directly call the parent\n",
    "class’s __init__ method with the child instance.\n",
    "Click here to view code image\n",
    "   class MyBaseClass(object):\n",
    "       def __init__(self, value):\n",
    "           self.value = value\n",
    "   class MyChildClass(MyBaseClass):\n",
    "       def __init__(self):\n",
    "MyBaseClass.__init__(self, 5)\n",
    "This approach works fine for simple hierarchies but breaks down in many cases.\n",
    "If your class is affected by multiple inheritance (something to avoid in general; see Item 26: “Use Multiple Inheritance Only for Mix-in Utility Classes”), calling the superclasses’ __init__ methods directly can lead to unpredictable behavior.\n",
    "One problem is that the __init__ call order isn’t specified across all subclasses. For example, here I define two parent classes that operate on the instance’s value field:\n",
    "   class TimesTwo(object):\n",
    "       def __init__(self):\n",
    "           self.value *= 2\n",
    "   class PlusFive(object):\n",
    "       def __init__(self):\n",
    "           self.value += 5\n",
    "This class defines its parent classes in one ordering.\n",
    "Click here to view code image\n",
    "   class OneWay(MyBaseClass, TimesTwo, PlusFive):\n",
    "       def __init__(self, value):\n",
    "           MyBaseClass.__init__(self, value)\n",
    "           TimesTwo.__init__(self)\n",
    "           PlusFive.__init__(self)\n",
    "￼￼￼￼￼￼￼￼￼\n",
    "And constructing it produces a result that matches the parent class ordering.\n",
    "Click here to view code image\n",
    "   foo = OneWay(5)\n",
    "   print(‘First ordering is (5 * 2) + 5 =’, foo.value)\n",
    "   >>>\n",
    "   First ordering is (5 * 2) + 5 = 15\n",
    "Here’s another class that defines the same parent classes but in a different ordering:\n",
    "Click here to view code image\n",
    "   class AnotherWay(MyBaseClass, PlusFive, TimesTwo):\n",
    "       def __init__(self, value):\n",
    "           MyBaseClass.__init__(self, value)\n",
    "           TimesTwo.__init__(self)\n",
    "           PlusFive.__init__(self)\n",
    "However, I left the calls to the parent class constructors PlusFive.__init__ and TimesTwo.__init__ in the same order as before, causing this class’s behavior not to match the order of the parent classes in its definition.\n",
    "Click here to view code image\n",
    "   bar = AnotherWay(5)\n",
    "   print(‘Second ordering still is’, bar.value)\n",
    "   >>>\n",
    "   Second ordering still is 15\n",
    "Another problem occurs with diamond inheritance. Diamond inheritance happens when a subclass inherits from two separate classes that have the same superclass somewhere in the hierarchy. Diamond inheritance causes the common superclass’s __init__ method to run multiple times, causing unexpected behavior. For example, here I define two child classes that inherit from MyBaseClass.\n",
    "Click here to view code image\n",
    "   class TimesFive(MyBaseClass):\n",
    "       def __init__(self, value):\n",
    "           MyBaseClass.__init__(self, value)\n",
    "           self.value *= 5\n",
    "   class PlusTwo(MyBaseClass):\n",
    "       def __init__(self, value):\n",
    "           MyBaseClass.__init__(self, value)\n",
    "           self.value += 2\n",
    "Then, I define a child class that inherits from both of these classes, making MyBaseClass the top of the diamond.\n",
    "Click here to view code image\n",
    "   class ThisWay(TimesFive, PlusTwo):\n",
    "       def __init__(self, value):\n",
    "           TimesFive.__init__(self, value)\n",
    "           PlusTwo.__init__(self, value)\n",
    "   foo = ThisWay(5)\n",
    "   print(‘Should be (5 * 5) + 2 = 27 but is’, foo.value)\n",
    "￼￼￼￼￼\n",
    "   >>>\n",
    "   Should be (5 * 5) + 2 = 27 but is 7\n",
    "The output should be 27 because (5 * 5) + 2 = 27. But the call to the second parent class’s constructor, PlusTwo.__init__, causes self.value to be reset back to 5 when MyBaseClass.__init__ gets called a second time.\n",
    "To solve these problems, Python 2.2 added the super built-in function and defined the method resolution order (MRO). The MRO standardizes which superclasses are initialized before others (e.g., depth-first, left-to-right). It also ensures that common superclasses in diamond hierarchies are only run once.\n",
    "Here, I create a diamond-shaped class hierarchy again, but this time I use super (in the Python 2 style) to initialize the parent class:\n",
    "Click here to view code image\n",
    "# Python 2\n",
    "   class TimesFiveCorrect(MyBaseClass):\n",
    "       def __init__(self, value):\n",
    "           super(TimesFiveCorrect, self).__init__(value)\n",
    "           self.value *= 5\n",
    "   class PlusTwoCorrect(MyBaseClass):\n",
    "       def __init__(self, value):\n",
    "           super(PlusTwoCorrect, self).__init__(value)\n",
    "           self.value += 2\n",
    "Now the top part of the diamond, MyBaseClass.__init__, is only run a single time. The other parent classes are run in the order specified in the class statement.\n",
    "Click here to view code image\n",
    "# Python 2\n",
    "   class GoodWay(TimesFiveCorrect, PlusTwoCorrect):\n",
    "       def __init__(self, value):\n",
    "           super(GoodWay, self).__init__(value)\n",
    "   foo = GoodWay(5)\n",
    "   print ‘Should be 5 * (5 + 2) = 35 and is’, foo.value\n",
    "   >>>\n",
    "   Should be 5 * (5 + 2) = 35 and is 35\n",
    "This order may seem backwards at first. Shouldn’t TimesFiveCorrect.__init__ have run first? Shouldn’t the result be (5 * 5) + 2 = 27? The answer is no. This ordering matches what the MRO defines for this class. The MRO ordering is available on a class method called mro.\n",
    "Click here to view code image\n",
    "   from pprint import pprint\n",
    "   pprint(GoodWay.mro())\n",
    "   >>>\n",
    "   [<class ‘__main__.GoodWay’>,\n",
    "   <class ‘__main__.TimesFiveCorrect’>,\n",
    "   <class ‘__main__.PlusTwoCorrect’>,\n",
    "   <class ‘__main__.MyBaseClass’>,\n",
    "￼￼￼\n",
    "<class ‘object’>]\n",
    "When I call GoodWay(5), it in turn calls TimesFiveCorrect.__init__, which calls PlusTwoCorrect.__init__, which calls MyBaseClass.__init__. Once this reaches the top of the diamond, then all of the initialization methods actually do their work in the opposite order from how their __init__ functions were called. MyBaseClass.__init__ assigns the value to 5. PlusTwoCorrect.__init__ adds 2 to make value equal 7. TimesFiveCorrect.__init__ multiplies it by 5 to make value equal 35.\n",
    "The super built-in function works well, but it still has two noticeable problems in Python 2:\n",
    "Its syntax is a bit verbose. You have to specify the class you’re in, the self object, the method name (usually __init__), and all the arguments. This construction can be confusing to new Python programmers.\n",
    "You have to specify the current class by name in the call to super. If you ever change the class’s name—a very common activity when improving a class hierarchy —you also need to update every call to super.\n",
    "Thankfully, Python 3 fixes these issues by making calls to super with no arguments equivalent to calling super with __class__ and self specified. In Python 3, you should always use super because it’s clear, concise, and always does the right thing.\n",
    "Click here to view code image\n",
    "   class Explicit(MyBaseClass):\n",
    "       def __init__(self, value):\n",
    "           super(__class__, self).__init__(value * 2)\n",
    "   class Implicit(MyBaseClass):\n",
    "       def __init__(self, value):\n",
    "           super().__init__(value * 2)\n",
    "   assert Explicit(10).value == Implicit(10).value\n",
    "This works because Python 3 lets you reliably reference the current class in methods using the __class__ variable. This doesn’t work in Python 2 because __class__ isn’t defined. You may guess that you could use self.__class__ as an argument to super, but this breaks because of the way super is implemented in Python 2.\n",
    "Things to Remember\n",
    "Python’s standard method resolution order (MRO) solves the problems of superclass initialization order and diamond inheritance.\n",
    "Always use the super built-in function to initialize parent classes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 26: Use Multiple Inheritance Only for Mix-in Utility Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is an object-oriented language with built-in facilities for making multiple inheritance tractable (see Item 25: “Initialize Parent Classes with super”). However, it’s better to avoid multiple inheritance altogether.\n",
    "\n",
    "If you find yourself desiring the convenience and encapsulation that comes with multiple inheritance, consider writing a mix-in instead. A mix-in is a small class that only defines a set of additional methods that a class should provide. Mix-in classes don’t define their own instance attributes nor require their __init__ constructor to be called.\n",
    "\n",
    "Writing mix-ins is easy because Python makes it trivial to inspect the current state of any object regardless of its type. Dynamic inspection lets you write generic functionality a single time, in a mix-in, that can be applied to many other classes. Mix-ins can be composed and layered to minimize repetitive code and maximize reuse.\n",
    "For example, say you want the ability to convert a Python object from its in-memory representation to a dictionary that’s ready for serialization. Why not write this functionality generically so you can use it with all of your classes?\n",
    "\n",
    "Here, I define an example mix-in that accomplishes this with a new public method that’s added to any class that inherits from it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Click here to view code image\n",
    "   class ToDictMixin(object):\n",
    "       def to_dict(self):\n",
    "           return self._traverse_dict(self.__dict__)\n",
    "The implementation details are straightforward and rely on dynamic attribute access using hasattr, dynamic type inspection with isinstance, and accessing the instance dictionary __dict__.\n",
    "Click here to view code image\n",
    "     def _traverse_dict(self, instance_dict):\n",
    "           output = {}\n",
    "           for key, value in instance_dict.items():\n",
    "               output[key] = self._traverse(key, value)\n",
    "           return output\n",
    "       def _traverse(self, key, value):\n",
    "           if isinstance(value, ToDictMixin):\n",
    "               return value.to_dict()\n",
    "           elif isinstance(value, dict):\n",
    "               return self._traverse_dict(value)\n",
    "           elif isinstance(value, list):\n",
    "               return [self._traverse(key, i) for i in value]\n",
    "           elif hasattr(value, ‘__dict__’):\n",
    "               return self._traverse_dict(value.__dict__)\n",
    "           else:\n",
    "return value\n",
    "Here, I define an example class that uses the mix-in to make a dictionary representation of a binary tree:\n",
    "￼￼￼￼￼\n",
    "Click here to view code image\n",
    "   class BinaryTree(ToDictMixin):\n",
    "       def __init__(self, value, left=None, right=None):\n",
    "           self.value = value\n",
    "           self.left = left\n",
    "           self.right = right\n",
    "Translating a large number of related Python objects into a dictionary becomes easy.\n",
    "Click here to view code image\n",
    "   tree = BinaryTree(10,\n",
    "       left=BinaryTree(7, right=BinaryTree(9)),\n",
    "       right=BinaryTree(13, left=BinaryTree(11)))\n",
    "   print(tree.to_dict())\n",
    "   >>>\n",
    "   {‘left’: {‘left’: None,\n",
    "             ‘right’: {‘left’: None, ‘right’: None, ‘value’: 9},\n",
    "             ‘value’: 7},\n",
    "   ‘right’: {‘left’: {‘left’: None, ‘right’: None, ‘value’: 11},\n",
    "              ‘right’: None,\n",
    "              ‘value’: 13},\n",
    "   ‘value’: 10}\n",
    "The best part about mix-ins is that you can make their generic functionality pluggable so behaviors can be overridden when required. For example, here I define a subclass of BinaryTree that holds a reference to its parent. This circular reference would cause the default implementation of ToDictMixin.to_dict to loop forever.\n",
    "Click here to view code image\n",
    "   class BinaryTreeWithParent(BinaryTree):\n",
    "       def __init__(self, value, left=None,\n",
    "                    right=None, parent=None):\n",
    "           super().__init__(value, left=left, right=right)\n",
    "           self.parent = parent\n",
    "The solution is to override the ToDictMixin._traverse method in the BinaryTreeWithParent class to only process values that matter, preventing cycles encountered by the mix-in. Here, I override the _traverse method to not traverse the parent and just insert its numerical value:\n",
    "Click here to view code image\n",
    "     def _traverse(self, key, value):\n",
    "           if (isinstance(value, BinaryTreeWithParent) and\n",
    "                   key == ‘parent’):\n",
    "               return value.value  # Prevent cycles\n",
    "           else:\n",
    "               return super()._traverse(key, value)\n",
    "Calling BinaryTreeWithParent.to_dict will work without issue because the circular referencing properties aren’t followed.\n",
    "Click here to view code image\n",
    "   root = BinaryTreeWithParent(10)\n",
    "   root.left = BinaryTreeWithParent(7, parent=root)\n",
    "   root.left.right = BinaryTreeWithParent(9, parent=root.left)\n",
    "   print(root.to_dict())\n",
    "￼￼￼￼￼\n",
    "   >>>\n",
    "   {‘left’: {‘left’: None,\n",
    "             ‘parent’: 10,\n",
    "             ‘right’: {‘left’: None,\n",
    "                       ‘parent’: 7,\n",
    "                       ‘right’: None,\n",
    "                       ‘value’: 9},\n",
    "             ‘value’: 7},\n",
    "   ‘parent’: None,\n",
    "   ‘right’: None,\n",
    "   ‘value’: 10}\n",
    "By defining BinaryTreeWithParent._traverse, I’ve also enabled any class that has an attribute of type BinaryTreeWithParent to automatically work with ToDictMixin.\n",
    "Click here to view code image\n",
    "   class NamedSubTree(ToDictMixin):\n",
    "       def __init__(self, name, tree_with_parent):\n",
    "           self.name = name\n",
    "           self.tree_with_parent = tree_with_parent\n",
    "   my_tree = NamedSubTree(‘foobar’, root.left.right)\n",
    "   print(my_tree.to_dict())  # No infinite loop\n",
    "   >>>\n",
    "   {‘name’: ‘foobar’,\n",
    "   ‘tree_with_parent’: {‘left’: None,\n",
    "                        ‘parent’: 7,\n",
    "                        ‘right’: None,\n",
    "                        ‘value’: 9}}\n",
    "Mix-ins can also be composed together. For example, say you want a mix-in that provides generic JSON serialization for any class. You can do this by assuming that a class provides a to_dict method (which may or may not be provided by the ToDictMixin class).\n",
    "Click here to view code image\n",
    "   class JsonMixin(object):\n",
    "       @classmethod\n",
    "       def from_json(cls, data):\n",
    "           kwargs = json.loads(data)\n",
    "           return cls(**kwargs)\n",
    "       def to_json(self):\n",
    "           return json.dumps(self.to_dict())\n",
    "Note how the JsonMixin class defines both instance methods and class methods. Mix- ins let you add either kind of behavior. In this example, the only requirements of the JsonMixin are that the class has a to_dict method and its __init__ method takes keyword arguments (see Item 19: “Provide Optional Behavior with Keyword Arguments”).\n",
    "This mix-in makes it simple to create hierarchies of utility classes that can be serialized to and from JSON with little boilerplate. For example, here I have a hierarchy of data classes representing parts of a datacenter topology:\n",
    "￼￼￼￼￼\n",
    "Click here to view code image\n",
    "   class DatacenterRack(ToDictMixin, JsonMixin):\n",
    "       def __init__(self, switch=None, machines=None):\n",
    "           self.switch = Switch(**switch)\n",
    "           self.machines = [\n",
    "               Machine(**kwargs) for kwargs in machines]\n",
    "   class Switch(ToDictMixin, JsonMixin):\n",
    "#...\n",
    "class Machine(ToDictMixin, JsonMixin): #...\n",
    "Serializing these classes to and from JSON is simple. Here, I verify that the data is able to be sent round-trip through serializing and deserializing:\n",
    "Click here to view code image\n",
    "   serialized = ”””{\n",
    "       “switch”: {“ports”: 5, “speed”: 1e9},\n",
    "       “machines”: [\n",
    "           {“cores”: 8, “ram”: 32e9, “disk”: 5e12},\n",
    "           {“cores”: 4, “ram”: 16e9, “disk”: 1e12},\n",
    "           {“cores”: 2, “ram”: 4e9, “disk”: 500e9}\n",
    "] }”””\n",
    "   deserialized = DatacenterRack.from_json(serialized)\n",
    "   roundtrip = deserialized.to_json()\n",
    "   assert json.loads(serialized) == json.loads(roundtrip)\n",
    "When you use mix-ins like this, it’s also fine if the class already inherits from JsonMixin higher up in the object hierarchy. The resulting class will behave the same way.\n",
    "Things to Remember\n",
    "Avoid using multiple inheritance if mix-in classes can achieve the same outcome.\n",
    "Use pluggable behaviors at the instance level to provide per-class customization when mix-in classes may require it.\n",
    "Compose mix-ins to create complex functionality from simple behaviors.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 27: Prefer Public Attributes Over Private Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Private attributes aren’t rigorously enforced by the Python compiler.\n",
    "* Plan from the beginning to allow subclasses to do more with your internal APIs and attributes instead of locking them out by default.\n",
    "* Use documentation of protected fields to guide subclasses instead of trying to force access control with private attributes.\n",
    "* Only consider using private attributes to avoid naming conflicts with subclasses that are out of your control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 28: Inherit from collections.abc for Custom Container Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of programming in Python is defining classes that contain data and describing how such objects relate to each other. Every Python class is a container of some kind, encapsulating attributes and functionality together. Python also provides built-in container types for managing data: lists, tuples, sets, and dictionaries.\n",
    "\n",
    "When you’re designing classes for simple use cases like sequences, it’s natural that you’d want to subclass Python’s built-in list type directly. For example, say you want to create your own custom list type that has additional methods for counting the frequency of its members.\n",
    "\n",
    "Things to Remember\n",
    "\n",
    "* Inherit directly from Python’s container types (like list or dict) for simple use cases.\n",
    "* Beware of the large number of methods required to implement custom container types correctly.\n",
    "* Have your custom container types inherit from the interfaces defined in collections.abc to ensure that your classes match required interfaces and behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaclasses and Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metaclasses are often mentioned in lists of Python’s features, but few understand what they accomplish in practice. The name metaclass vaguely implies a concept above and beyond a class. Simply put, metaclasses let you intercept Python’s class statement and provide special behavior each time a class is defined.\n",
    "\n",
    "Similarly mysterious and powerful are Python’s built-in features for dynamically customizing attribute accesses. Along with Python’s object-oriented constructs, these facilities provide wonderful tools to ease the transition from simple classes to complex ones.\n",
    "\n",
    "However, with these powers come many pitfalls. Dynamic attributes enable you to override objects and cause unexpected side effects. Metaclasses can create extremely bizarre behaviors that are unapproachable to newcomers. It’s important that you follow the rule of least surprise and only use these mechanisms to implement well-understood idioms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 29: Use Plain Attributes Instead of Get and Set Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programmers coming to Python from other languages may naturally try to implement explicit getter and setter methods in their classes.\n",
    "```  \n",
    "  class OldResistor(object):\n",
    "       def __init__(self, ohms):\n",
    "           self._ohms = ohms\n",
    "       def get_ohms(self):\n",
    "           return self._ohms\n",
    "       def set_ohms(self, ohms):\n",
    "self._ohms = ohms\n",
    "```\n",
    "Using these setters and getters is simple, but it’s not Pythonic.\n",
    "\n",
    "In Python, however, you almost never need to implement explicit setter or getter methods. Instead, you should always start your implementations with simple public attributes.\n",
    "```\n",
    "Click here to view code image\n",
    "   class Resistor(object):\n",
    "       def __init__(self, ohms):\n",
    "           self.ohms = ohms\n",
    "           self.voltage = 0\n",
    "           self.current = 0\n",
    "   r1 = Resistor(50e3)\n",
    "   r1.ohms = 10e3\n",
    "   ```\n",
    "These make operations like incrementing in place natural and clear. r1.ohms += 5e3\n",
    "Later, if you decide you need special behavior when an attribute is set, you can migrate to the @property decorator and its corresponding setter attribute. Here, I define a new subclass of Resistor that lets me vary the current by assigning the voltage property. Note that in order to work properly the name of both the setter and getter methods must match the intended property name.\n",
    "```Click here to view code image\n",
    "   class VoltageResistance(Resistor):\n",
    "       def __init__(self, ohms):\n",
    "           super().__init__(ohms)\n",
    "           self._voltage = 0\n",
    "       @property\n",
    "       def voltage(self):\n",
    "           return self._voltage\n",
    "       @voltage.setter\n",
    "       def voltage(self, voltage):\n",
    "           self._voltage = voltage\n",
    "           self.current = self._voltage / self.ohms\n",
    "```\n",
    "Now, assigning the voltage property will run the voltage setter method, updating the current property of the object to match."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest shortcoming of @property is that the methods for an attribute can only be shared by subclasses. Unrelated classes can’t share the same implementation. However, Python also supports descriptors (see Item 31: “Use Descriptors for Reusable @property Methods”) that enable reusable property logic and many other use cases.\n",
    "\n",
    "Finally, when you use @property methods to implement setters and getters, be sure that the behavior you implement is not surprising. For example, don’t set other attributes in getter property methods.\n",
    "\n",
    "The best policy is to only modify related object state in @property.setter methods. Be sure to avoid any other side effects the caller may not expect beyond the object, such as importing modules dynamically, running slow helper functions, or making expensive database queries. Users of your class will expect its attributes to be like any other Python object: quick and easy. Use normal methods to do anything more complex or slow.\n",
    "Things to Remember\n",
    "\n",
    "Define new class interfaces using simple public attributes, and avoid set and get methods.\n",
    "Use @property to define special behavior when attributes are accessed on your objects, if necessary.\n",
    "Follow the rule of least surprise and avoid weird side effects in your @property methods.\n",
    "Ensure that @property methods are fast; do slow or complex work using normal methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 30: Consider @property Instead of Refactoring Attributes\n",
    "The built-in @property decorator makes it easy for simple accesses of an instance’s attributes to act smarter (see Item 29: “Use Plain Attributes Instead of Get and Set Methods”). One advanced but common use of @property is transitioning what was once a simple numerical attribute into an on-the-fly calculation. This is extremely helpful because it lets you migrate all existing usage of a class to have new behaviors without rewriting any of the call sites. It also provides an important stopgap for improving your interfaces over time.\n",
    "\n",
    "For example, say you want to implement a leaky bucket quota using plain Python objects. Here, the Bucket class represents how much quota remains and the duration for which the quota will be available:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best part is that the code using Bucket.quota doesn’t have to change or know that the class has changed. New usage of Bucket can do the right thing and access max_quota and quota_consumed directly.\n",
    "I especially like @property because it lets you make incremental progress toward a better data model over time. Reading the Bucket example above, you may have thought to yourself, “fill and deduct should have been implemented as instance methods in\n",
    "￼￼\n",
    "the first place.” Although you’re probably right (see Item 22: “Prefer Helper Classes Over Bookkeeping with Dictionaries and Tuples”), in practice there are many situations in which objects start with poorly defined interfaces or act as dumb data containers. This happens when code grows over time, scope increases, multiple authors contribute without anyone considering long-term hygiene, etc.\n",
    "\n",
    "@property is a tool to help you address problems you’ll come across in real-world code. Don’t overuse it. When you find yourself repeatedly extending @property methods, it’s probably time to refactor your class instead of further paving over your code’s poor design.\n",
    "\n",
    "Things to Remember\n",
    "* Use @property to give existing instance attributes new functionality. \n",
    "* Make incremental progress toward better data models by using @property.\n",
    "* Consider refactoring a class and all call sites when you find yourself using @property too heavily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 31: Use Descriptors for Reusable @property Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Reuse the behavior and validation of @property methods by defining your own descriptor classes.\n",
    "* Use WeakKeyDictionary to ensure that your descriptor classes don’t cause memory leaks.\n",
    "* Don’t get bogged down trying to understand exactly how __getattribute__ uses the descriptor protocol for getting and setting attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 32: Use __getattr__, __getattribute__, and __setattr__ for Lazy Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python’s language hooks make it easy to write generic code for gluing systems together. For example, say you want to represent the rows of your database as Python objects. Your database has its schema set. Your code that uses objects corresponding to those rows must also know what your database looks like. However, in Python, the code that connects your Python objects to the database doesn’t need to know the schema of your rows; it can be generic.\n",
    "\n",
    "How is that possible? Plain instance attributes, @property methods, and descriptors can’t do this because they all need to be defined in advance. Python makes this dynamic behavior possible with the \\__getattr\\__ special method. If your class defines \\__getattr\\__, that method is called every time an attribute can’t be found in an object’s instance dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LazyDB(object):\n",
    "       def __init__(self):\n",
    "           self.exists = 5 \n",
    "       def __getattr__(self, name):\n",
    "           value = 'Value for %s' % name\n",
    "           setattr(self, name, value)\n",
    "           return value\n",
    "\n",
    "# Here, I access the missing property foo. This causes Python to call the __getattr__ method above, \n",
    "# which mutates the instance dictionary __dict__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: {'exists': 5}\n",
      "foo:    Value for foo\n",
      "After:  {'exists': 5, 'foo': 'Value for foo'}\n",
      "val2  Value for val2\n",
      "After:  {'exists': 5, 'foo': 'Value for foo', 'val2': 'Value for val2'}\n"
     ]
    }
   ],
   "source": [
    "data = LazyDB()\n",
    "print('Before:', data.__dict__)\n",
    "print('foo:   ', data.foo)\n",
    "print('After: ', data.__dict__)\n",
    "print('val2 ', data.val2)\n",
    "print('After: ', data.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists: 5\n",
      "Called __getattr__(foo)\n",
      "foo:    Value for foo\n",
      "foo:    Value for foo\n"
     ]
    }
   ],
   "source": [
    "class LoggingLazyDB(LazyDB):\n",
    "       def __getattr__(self, name):\n",
    "           print('Called __getattr__(%s)' % name)\n",
    "           return super().__getattr__(name)\n",
    "data = LoggingLazyDB()\n",
    "print('exists:', data.exists)\n",
    "print('foo:   ', data.foo)\n",
    "print('foo:   ', data.foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exists attribute is present in the instance dictionary, so __getattr__ is never called for it. The foo attribute is not in the instance dictionary initially, so __getattr__ is called the first time. But the call to __getattr__ for foo also does a setattr, which populates foo in the instance dictionary. This is why the second time I access foo there isn’t a call to __getattr__.\n",
    "\n",
    "This behavior is especially helpful for use cases like lazily accessing schemaless data. __getattr__ runs once to do the hard work of loading a property; all subsequent accesses retrieve the existing result.\n",
    "Say you also want transactions in this database system. The next time the user accesses a property, you want to know whether the corresponding row in the database is still valid and whether the transaction is still open. The __getattr__ hook won’t let you do this reliably because it will use the object’s instance dictionary as the fast path for existing attributes.\n",
    "\n",
    "To enable this use case, Python has another language hook called __getattribute__. This special method is called every time an attribute is accessed on an object, even in cases where it does exist in the attribute dictionary. This enables you to do things like\n",
    "￼￼\n",
    "check global transaction state on every property access. Here, I define ValidatingDB to log each time __getattribute__ is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValidatingDB(object):\n",
    "       def __init__(self):\n",
    "           self.exists = 5\n",
    "       def __getattribute__(self, name):\n",
    "           print('Called __getattribute__(%s)' % name)\n",
    "           try:\n",
    "               return super().__getattribute__(name)\n",
    "           except AttributeError:\n",
    "               value = 'Value for %s' % name\n",
    "               setattr(self, name, value)\n",
    "               return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called __getattribute__(exists)\n",
      "exists: 5\n",
      "Called __getattribute__(foo)\n",
      "foo:    Value for foo\n",
      "Called __getattribute__(foo)\n",
      "foo:    Value for foo\n"
     ]
    }
   ],
   "source": [
    "data = ValidatingDB()\n",
    "print('exists:', data.exists)\n",
    "print('foo:   ', data.foo)\n",
    "print('foo:   ', data.foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Use __getattr__ and __setattr__ to lazily load and save attributes for an object.\n",
    "￼￼￼￼\n",
    "* Understand that __getattr__ only gets called once when accessing a missing attribute, whereas __getattribute__ gets called every time an attribute is accessed.\n",
    "\n",
    "* Avoid infinite recursion in __getattribute__ and __setattr__ by using methods from super() (i.e., the object class) to access instance attributes directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 33: Validate Subclasses with Metaclasses\n",
    "One of the simplest applications of metaclasses is verifying that a class was defined correctly. When you’re building a complex class hierarchy, you may want to enforce style, require overriding methods, or have strict relationships between class attributes. Metaclasses enable these use cases by providing a reliable way to run your validation code each time a new subclass is defined.\n",
    "\n",
    "Often a class’s validation code runs in the __init__ method, when an object of the class’s type is constructed (see Item 28: “Inherit from collections.abc for Custom Container Types” for an example). Using metaclasses for validation can raise errors much earlier.\n",
    "\n",
    "Before I get into how to define a metaclass for validating subclasses, it’s important to understand the metaclass action for standard objects. A metaclass is defined by inheriting from type. In the default case, a metaclass receives the contents of associated class statements in its __new__ method. Here, you can modify the class information before the type is actually constructed:\n",
    "```\n",
    "Click here to view code image\n",
    "   class Meta(type):\n",
    "       def __new__(meta, name, bases, class_dict):\n",
    "           print((meta, name, bases, class_dict))\n",
    "           return type.__new__(meta, name, bases, class_dict)\n",
    "   class MyClass(object, metaclass=Meta):\n",
    "       stuff = 123\n",
    "       def foo(self):\n",
    "           pass\n",
    "```\n",
    "The metaclass has access to the name of the class, the parent classes it inherits from, and all of the class attributes that were defined in the class’s body.\n",
    "```\n",
    "Click here to view code image\n",
    "   >>>\n",
    "   (<class ‘__main__.Meta’>,\n",
    "    ‘MyClass’,\n",
    "    (<class ‘object’>,),\n",
    "    {‘__module__’: ‘__main__’,\n",
    "     ‘__qualname__’: ‘MyClass’,\n",
    "     ‘foo’: <function MyClass.foo at 0x102c7dd08>,\n",
    "     ‘stuff’: 123})\n",
    "Python 2 has slightly different syntax and specifies a metaclass using the __metaclass__ class attribute. The Meta.__new__ interface is the same.\n",
    "￼￼￼￼￼￼￼￼￼\n",
    "Click here to view code image\n",
    "# Python 2\n",
    "   class Meta(type):\n",
    "       def __new__(meta, name, bases, class_dict):\n",
    "#...\n",
    "class MyClassInPython2(object): \n",
    "__metaclass__ = Meta\n",
    "#...\n",
    "```\n",
    "You can add functionality to the Meta.__new__ method in order to validate all of the parameters of a class before it’s defined. For example, say you want to represent any type of multisided polygon. You can do this by defining a special validating metaclass and using it in the base class of your polygon class hierarchy. Note that it’s important not to apply the same validation to the base class.\n",
    "```\n",
    "Click here to view code image\n",
    "   class ValidatePolygon(type):\n",
    "       def __new__(meta, name, bases, class_dict):\n",
    "           # Don’t validate the abstract Polygon class\n",
    "           if bases != (object,):\n",
    "               if class_dict[‘sides’] < 3:\n",
    "                   raise ValueError(‘Polygons need 3+ sides’)\n",
    "           return type.__new__(meta, name, bases, class_dict)\n",
    "   class Polygon(object, metaclass=ValidatePolygon):\n",
    "       sides = None  # Specified by subclasses\n",
    "       @classmethod\n",
    "       def interior_angles(cls):\n",
    "           return (cls.sides - 2) * 180\n",
    "   class Triangle(Polygon):\n",
    "sides = 3\n",
    "\n",
    "```\n",
    "If you try to define a polygon with fewer than three sides, the validation will cause the class statement to fail immediately after the class statement body. This means your program will not even be able to start running when you define such a class.\n",
    "```\n",
    "Click here to view code image\n",
    "   print(‘Before class’)\n",
    "   class Line(Polygon):\n",
    "       print(‘Before sides’)\n",
    "       sides = 1\n",
    "       print(‘After sides’)\n",
    "   print(‘After class’)\n",
    "   >>>\n",
    "   Before class\n",
    "   Before sides\n",
    "   After sides\n",
    "   Traceback ...\n",
    "   ValueError: Polygons need 3+ sides\n",
    "```\n",
    "￼￼￼\n",
    "Things to Remember\n",
    "* Use metaclasses to ensure that subclasses are well formed at the time they are defined, before objects of their type are constructed.\n",
    "* Metaclasses have slightly different syntax in Python 2 vs. Python 3.\n",
    "* The __new__ method of metaclasses is run after the class statement’s entire body has been processed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 34: Register Class Existence with Metaclasses\n",
    "\n",
    "Another common use of metaclasses is to automatically register types in your program. Registration is useful for doing reverse lookups, where you need to map a simple identifier back to a corresponding class.\n",
    "\n",
    "For example, say you want to implement your own serialized representation of a Python object using JSON. You need a way to take an object and turn it into a JSON string. Here, I do this generically by defining a base class that records the constructor parameters and turns them into a JSON dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Serializable(object):\n",
    "       def __init__(self, *args):\n",
    "           self.args = args\n",
    "       def serialize(self):\n",
    "            return json.dumps({'args': self.args})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Point2D(Serializable):\n",
    "       def __init__(self, x, y):\n",
    "           super().__init__(x, y)\n",
    "           self.x = x\n",
    "           self.y = y\n",
    "       def __repr__(self):\n",
    "           return 'Point2D(%d, %d)' % (self.x, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object:     Point2D(5, 3)\n",
      "Serialized: {\"args\": [5, 3]}\n"
     ]
    }
   ],
   "source": [
    "point = Point2D(5, 3)\n",
    "print('Object:    ', point)\n",
    "print('Serialized:', point.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using metaclasses for class registration ensures that you’ll never miss a class as long as the inheritance tree is right. This works well for serialization, as I’ve shown, and also applies to database object-relationship mappings (ORMs), plug-in systems, and system hooks.\n",
    "Things to Remember\n",
    "* Class registration is a helpful pattern for building modular Python programs.\n",
    "* Metaclasses let you run registration code automatically each time your base class is subclassed in a program.\n",
    "* Using metaclasses for class registration avoids errors by ensuring that you never miss a registration call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Concurrency and Parallelism\n",
    " \n",
    " Concurrency is when a computer does many different things seemingly at the same time. For example, on a computer with one CPU core, the operating system will rapidly change which program is running on the single processor. This interleaves execution of the programs, providing the illusion that the programs are running simultaneously.\n",
    " \n",
    "Parallelism is actually doing many different things at the same time. Computers with multiple CPU cores can execute multiple programs simultaneously. Each CPU core runs the instructions of a separate program, allowing each program to make forward progress during the same instant.\n",
    "Within a single program, concurrency is a tool that makes it easier for programmers to solve certain types of problems. Concurrent programs enable many distinct paths of execution to make forward progress in a way that seems to be both simultaneous and independent.\n",
    "\n",
    "The key difference between parallelism and concurrency is speedup. When two distinct paths of execution in a program make forward progress in parallel, the time it takes to do the total work is cut in half; the speed of execution is faster by a factor of two. In contrast, concurrent programs may run thousands of separate paths of execution seemingly in parallel but provide no speedup for the total work.\n",
    "\n",
    "Python makes it easy to write concurrent programs. Python can also be used to do parallel work through system calls, subprocesses, and C-extensions. But it can be very difficult to make concurrent Python code truly run in parallel. It’s important to understand how to best utilize Python in these subtly different situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 36: Use subprocess to Manage Child Processes\n",
    "\n",
    "Python has battle-hardened libraries for running and managing child processes. This makes Python a great language for gluing other tools together, such as command-line utilities. When existing shell scripts get complicated, as they often do over time, graduating them to a rewrite in Python is a natural choice for the sake of readability and maintainability.\n",
    "\n",
    "Child processes started by Python are able to run in parallel, enabling you to use Python to consume all of the CPU cores of your machine and maximize the throughput of your programs. Although Python itself may be CPU bound (see Item 37: “Use Threads for Blocking I/O, Avoid for Parallelism”), it’s easy to use Python to drive and coordinate CPU-intensive workloads.\n",
    "\n",
    "Python has had many ways to run subprocesses over the years, including popen, popen2, and os.exec*. With the Python of today, the best and simplest choice for managing child processes is to use the subprocess built-in module.\n",
    "\n",
    "Running a child process with subprocess is simple. Here, the Popen constructor starts the process. The communicate method reads the child process’s output and waits for termination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the child!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "proc = subprocess.Popen(['echo', 'Hello from the child!'],stdout=subprocess.PIPE)\n",
    "out, err = proc.communicate()\n",
    "print(out.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoupling the child process from the parent means that the parent process is free to run many child processes in parallel. You can do this by starting all the child processes together upfront."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 0.135 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "def run_sleep(period):\n",
    "       proc = subprocess.Popen(['sleep', str(period)])\n",
    "       return proc\n",
    "start = t.time()\n",
    "procs = []\n",
    "for _ in range(10):\n",
    "    proc = run_sleep(0.1)\n",
    "    procs.append(proc)\n",
    "    \n",
    "for proc in procs:\n",
    "    proc.communicate()\n",
    "end = t.time()\n",
    "print('Finished in %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pipe data from your Python program into a subprocess and retrieve its output. This allows you to utilize other programs to do work in parallel. For example, say you want to use the openssl command-line tool to encrypt some data. Starting the child process with command-line arguments and I/O pipes is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xe6hw\\xe60\\xf8\\xfa\\xca\\xf6q'\n",
      "b'(\\xf70\\xe2 \\xe4\\x89\\xc8$ '\n",
      "b'9B\\x0b\\xa9\\x9a\\xb6-\\xad\\xf4\"'\n"
     ]
    }
   ],
   "source": [
    "def run_openssl(data):\n",
    "    env = os.environ.copy()\n",
    "    env['password'] = b'\\xe24U\\n\\xd0Ql3S\\x11'\n",
    "    proc = subprocess.Popen(\n",
    "        ['openssl', 'enc', '-des3', '-pass', 'env:password'],\n",
    "        env=env,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE)\n",
    "    proc.stdin.write(data)\n",
    "    proc.stdin.flush()  # Ensure the child gets input\n",
    "    return proc\n",
    "\n",
    "procs = []\n",
    "import os\n",
    "for _ in range(3):\n",
    "    data = os.urandom(10)\n",
    "    proc = run_openssl(data)\n",
    "    procs.append(proc)\n",
    "\n",
    "for proc in procs:\n",
    "    out, err = proc.communicate()\n",
    "    print(out[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'2581a5453ea15c28c3b7aee7067083c7'\n",
      "b'd9663f1017a9fc7b9294ef5fde8c1699'\n",
      "b'f83e17279f8d747a5cc667888431c1f8'\n"
     ]
    }
   ],
   "source": [
    "def run_md5(input_stdin):\n",
    "    proc = subprocess.Popen(\n",
    "        ['md5'],\n",
    "        stdin=input_stdin,\n",
    "        stdout=subprocess.PIPE)\n",
    "    return proc\n",
    "input_procs = []\n",
    "hash_procs = []\n",
    "for _ in range(3):\n",
    "    data = os.urandom(10)\n",
    "    proc = run_openssl(data)\n",
    "    input_procs.append(proc)\n",
    "    hash_proc = run_md5(proc.stdout)\n",
    "    hash_procs.append(hash_proc)\n",
    "\n",
    "for proc in input_procs:\n",
    "    proc.communicate()\n",
    "for proc in hash_procs:\n",
    "    out, err = proc.communicate()\n",
    "    print(out.strip()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’re worried about the child processes never finishing or somehow blocking on input or output pipes, then be sure to pass the timeout parameter to the communicate method. This will cause an exception to be raised if the child process hasn’t responded within a time period, giving you a chance to terminate the misbehaving child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit status -15\n"
     ]
    }
   ],
   "source": [
    "proc = run_sleep(10)\n",
    "try:\n",
    "    proc.communicate(timeout=0.1)\n",
    "except subprocess.TimeoutExpired:\n",
    "    proc.terminate()\n",
    "    proc.wait()\n",
    "print('Exit status', proc.poll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Use the subprocess module to run child processes and manage their input and output streams.\n",
    "* Child processes run in parallel with the Python interpreter, enabling you to maximize your CPU usage.\n",
    "* Use the timeout parameter with communicate to avoid deadlocks and hanging child processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 37: Use Threads for Blocking I/O, Avoid for Parallelism\n",
    "\n",
    "The standard implementation of Python is called CPython. CPython runs a Python program in two steps. First, it parses and compiles the source text into bytecode. Then, it runs the bytecode using a stack-based interpreter. The bytecode interpreter has state that must be maintained and coherent while the Python program executes. Python enforces coherence with a mechanism called the global interpreter lock (GIL).\n",
    "\n",
    "Essentially, the GIL is a mutual-exclusion lock (mutex) that prevents CPython from being affected by preemptive multithreading, where one thread takes control of a program by interrupting another thread. Such an interruption could corrupt the interpreter state if it comes at an unexpected time. The GIL prevents these interruptions and ensures that every bytecode instruction works correctly with the CPython implementation and its C- extension modules.\n",
    "\n",
    "The GIL has an important negative side effect. With programs written in languages like C++ or Java, having multiple threads of execution means your program could utilize multiple CPU cores at the same time. Although Python supports multiple threads of execution, the GIL causes only one of them to make forward progress at a time. This means that when you reach for threads to do parallel computation and speed up your Python programs, you will be sorely disappointed.\n",
    "For example, say you want to do something computationally intensive with Python. I’ll use a naive number factorization algorithm as a proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.277 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "def factorize(number):\n",
    "    for i in range(1, number + 1):\n",
    "        if number % i == 0:\n",
    "            yield i\n",
    "            \n",
    "numbers = [2139079, 1214759, 1516637, 1852285]\n",
    "start = t.time()\n",
    "for number in numbers:\n",
    "    list(factorize(number))\n",
    "end = t.time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using multiple threads to do this computation would make sense in other languages because you could take advantage of all of the CPU cores of your computer. Let me try that in Python. Here, I define a Python thread for doing the same computation as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.445 seconds\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "class FactorizeThread(Thread):\n",
    "    def __init__(self, number):\n",
    "        super().__init__()\n",
    "        self.number = number\n",
    "    def run(self):\n",
    "        self.factors = list(factorize(self.number))\n",
    "\n",
    "start = t.time()\n",
    "threads = []\n",
    "for number in numbers:\n",
    "    thread = FactorizeThread(number)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "end = t.time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What’s surprising is that this takes even longer than running factorize in serial. With one thread per number, you may expect less than a 4× speedup in other languages due to the overhead of creating threads and coordinating with them. You may expect only a 2× speedup on the dual-core machine I used to run this code. But you would never expect the performance of these threads to be worse when you have multiple CPUs to utilize. This demonstrates the effect of the GIL on programs running in the standard CPython interpreter.\n",
    "\n",
    "There are ways to get CPython to utilize multiple cores, but it doesn’t work with the standard Thread class (see Item 41: “Consider concurrent.futures for True Parallelism”) and it can require substantial effort. Knowing these limitations you may wonder, why does Python support threads at all? There are two good reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, multiple threads make it easy for your program to seem like it’s doing multiple things at the same time. Managing the juggling act of simultaneous tasks is difficult to\n",
    "￼￼￼￼￼￼￼￼\n",
    "implement yourself (see Item 40: “Consider Coroutines to Run Many Functions Concurrently” for an example). With threads, you can leave it to Python to run your functions seemingly in parallel. This works because CPython ensures a level of fairness between Python threads of execution, even though only one of them makes forward progress at a time due to the GIL.\n",
    "\n",
    "The second reason Python supports threads is to deal with blocking I/O, which happens when Python does certain types of system calls. System calls are how your Python program asks your computer’s operating system to interact with the external environment on your behalf. Blocking I/O includes things like reading and writing files, interacting with networks, communicating with devices like displays, etc. Threads help you handle blocking I/O by insulating your program from the time it takes for the operating system to respond to your requests.\n",
    "\n",
    "For example, say you want to send a signal to a remote-controlled helicopter through a serial port. I’ll use a slow system call (select) as a proxy for this activity. This function asks the operating system to block for 0.1 second and then return control to my program, similar to what would happen when using a synchronous serial port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.522 seconds\n"
     ]
    }
   ],
   "source": [
    "import select\n",
    "def slow_systemcall():\n",
    "    select.select([], [], [], 0.1)\n",
    "\n",
    "start = t.time()\n",
    "for _ in range(5):\n",
    "    slow_systemcall()\n",
    "end = t.time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that while the slow_systemcall function is running, my program can’t make any other progress. My program’s main thread of execution is blocked on the select system call. This situation is awful in practice. You need to be able to compute your helicopter’s next move while you’re sending it a signal, otherwise it’ll crash. When you find yourself needing to do blocking I/O and computation simultaneously, it’s time to consider moving your system calls to threads.\n",
    "\n",
    "Here, I run multiple invocations of the slow_systemcall function in separate threads. This would allow you to communicate with multiple serial ports (and helicopters) at the same time, while leaving the main thread to do whatever computation is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.104 seconds\n"
     ]
    }
   ],
   "source": [
    "start = t.time()\n",
    "threads = []\n",
    "for _ in range(5):\n",
    "    thread = Thread(target=slow_systemcall)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def compute_helicopter_location(index): #...\n",
    "    pass\n",
    "for i in range(5):\n",
    "    compute_helicopter_location(i)\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "end = t.time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parallel time is 5× less than the serial time. This shows that the system calls will all run in parallel from multiple Python threads even though they’re limited by the GIL. The GIL prevents my Python code from running in parallel, but it has no negative effect on system calls. This works because Python threads release the GIL just before they make system calls and reacquire the GIL as soon as the system calls are done.\n",
    "\n",
    "There are many other ways to deal with blocking I/O besides threads, such as the asyncio built-in module, and these alternatives have important benefits. But these options also require extra work in refactoring your code to fit a different model of execution (see Item 40: “Consider Coroutines to Run Many Functions Concurrently”). Using threads is the simplest way to do blocking I/O in parallel with minimal changes to your program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Python threads can’t run bytecode in parallel on multiple CPU cores because of the global interpreter lock (GIL).\n",
    "* Python threads are still useful despite the GIL because they provide an easy way to do multiple things at seemingly the same time.\n",
    "* Use Python threads to make multiple system calls in parallel. This allows you to do blocking I/O at the same time as computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 38: Use Lock to Prevent Data Races in Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning about the global interpreter lock (GIL) (see Item 37: “Use Threads for Blocking I/O, Avoid for Parallelism”), many new Python programmers assume they can forgo using mutual-exclusion locks (mutexes) in their code altogether. If the GIL is already preventing Python threads from running on multiple CPU cores in parallel, it must also act as a lock for a program’s data structures, right? Some testing on types like lists and dictionaries may even show that this assumption appears to hold.\n",
    "\n",
    "But beware, this is truly not the case. The GIL will not protect you. Although only one\n",
    "￼￼￼￼￼￼￼￼￼\n",
    "Python thread runs at a time, a thread’s operations on data structures can be interrupted between any two bytecode instructions in the Python interpreter. This is dangerous if you access the same objects from multiple threads simultaneously. The invariants of your data structures could be violated at practically any time because of these interruptions, leaving your program in a corrupted state.\n",
    "\n",
    "For example, say you want to write a program that counts many things in parallel, like sampling light levels from a whole network of sensors. If you want to determine the total number of light samples over time, you can aggregate them with a new class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter should be 500000, found 359522\n"
     ]
    }
   ],
   "source": [
    "class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "    def increment(self, offset):\n",
    "        self.count += offset\n",
    "\n",
    "def worker(sensor_index, how_many, counter):\n",
    "    for _ in range(how_many):\n",
    "        # Read from the sensor #... \n",
    "        counter.increment(1)\n",
    "\n",
    "def run_threads(func, how_many, counter):\n",
    "    threads = []\n",
    "    for i in range(5):\n",
    "        args = (i, how_many, counter)\n",
    "        thread = Thread(target=func, args=args)\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "how_many = 10**5\n",
    "counter = Counter()\n",
    "run_threads(worker, how_many, counter)\n",
    "print('Counter should be %d, found %d' %(5 * how_many, counter.count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this result is way off! What happened here? How could something so simple go so wrong, especially since only one Python interpreter thread can run at a time?\n",
    "\n",
    "￼￼￼￼\n",
    "The Python interpreter enforces fairness between all of the threads that are executing to ensure they get a roughly equal amount of processing time. To do this, Python will suspend a thread as it’s running and will resume another thread in turn. The problem is that you don’t know exactly when Python will suspend your threads. A thread can even be paused seemingly halfway through what looks like an atomic operation. That’s what happened in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Counter object’s increment method looks simple. \n",
    "```\n",
    "counter.count += offset\n",
    "```\n",
    "But the += operator used on an object attribute actually instructs Python to do three separate operations behind the scenes. The statement above is equivalent to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "value = getattr(counter, ‘count’)\n",
    "result = value + offset\n",
    "setattr(counter, ‘count’, result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent data races like these and other forms of data structure corruption, Python includes a robust set of tools in the threading built-in module. The simplest and most useful of them is the Lock class, a mutual-exclusion lock (mutex).\n",
    "\n",
    "By using a lock, I can have the Counter class protect its current value against simultaneous access from multiple threads. Only one thread will be able to acquire the lock at a time. Here, I use a with statement to acquire and release the lock; this makes it easier to see which code is executing while the lock is held (see Item 43: “Consider contextlib and with Statements for Reusable try/finally Behavior” for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter should be 500000, found 500000\n"
     ]
    }
   ],
   "source": [
    "from threading import Lock\n",
    "class LockingCounter(object):\n",
    "    def __init__(self):\n",
    "        self.lock = Lock()\n",
    "        self.count = 0\n",
    "    def increment(self, offset):\n",
    "        with self.lock:\n",
    "            self.count += offset\n",
    "\n",
    "\n",
    "counter = LockingCounter()\n",
    "run_threads(worker, how_many, counter)\n",
    "print('Counter should be %d, found %d' %\n",
    "      (5 * how_many, counter.count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Even though Python has a global interpreter lock, you’re still responsible for protecting against data races between the threads in your programs.\n",
    "* Your programs will corrupt their data structures if you allow multiple threads to modify the same objects without locks.\n",
    "* The Lock class in the threading built-in module is Python’s standard mutual exclusion lock implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 39: Use Queue to Coordinate Work Between Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python programs that do many things concurrently often need to coordinate their work.\n",
    "One of the most useful arrangements for concurrent work is a pipeline of functions.\n",
    "\n",
    "A pipeline works like an assembly line used in manufacturing. Pipelines have many phases in serial with a specific function for each phase. New pieces of work are constantly added to the beginning of the pipeline. Each function can operate concurrently on the piece of work in its phase. The work moves forward as each function completes until there are no phases remaining. This approach is especially good for work that includes blocking I/O or subprocesses—activities that can easily be parallelized using Python (see Item 37: “Use Threads for Blocking I/O, Avoid for Parallelism”).\n",
    "\n",
    "For example, say you want to build a system that will take a constant stream of images from your digital camera, resize them, and then add them to a photo gallery online. Such a program could be split into three phases of a pipeline. New images are retrieved in the first phase. The downloaded images are passed through the resize function in the second phase. The resized images are consumed by the upload function in the final phase.\n",
    "\n",
    "Imagine you had already written Python functions that execute the phases: download, resize, upload. How do you assemble a pipeline to do the work concurrently?\n",
    "\n",
    "The first thing you need is a way to hand off work between the pipeline phases. This can be modeled as a thread-safe producer-consumer queue (see Item 38: “Use Lock to Prevent Data Races in Threads” to understand the importance of thread safety in Python; see Item 46: “Use Built-in Algorithms and Data Structures” for the deque class).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The producer, your digital camera, adds new images to the end of the list of pending items.\n",
    "The consumer, the first phase of your processing pipeline, removes images from the front of the list of pending items.\n",
    "\n",
    "Here, I represent each phase of the pipeline as a Python thread that takes work from one queue like this, runs a function on it, and puts the result on another queue. I also track how many times the worker has checked for new input and how much work it’s completed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "class MyQueue(object):\n",
    "    def __init__(self):\n",
    "        self.items = deque()\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def put(self, item):\n",
    "        with self.lock:\n",
    "            self.items.append(item)\n",
    "\n",
    "    def get(self):\n",
    "        with self.lock:\n",
    "            return self.items.popleft()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Worker(Thread):\n",
    "    def __init__(self, func, in_queue, out_queue):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "        self.in_queue = in_queue\n",
    "        self.out_queue = out_queue\n",
    "        self.polled_count = 0\n",
    "        self.work_done = 0\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            self.polled_count += 1\n",
    "            try:\n",
    "                item = self.in_queue.get()\n",
    "            except IndexError:\n",
    "                sleep(0.01)  # No work to do\n",
    "            else:\n",
    "                result = self.func(item)\n",
    "                self.out_queue.put(result)\n",
    "                self.work_done += 1\n",
    "\n",
    "def download():\n",
    "    pass\n",
    "def resize():\n",
    "    pass\n",
    "def upload():\n",
    "    pass\n",
    "\n",
    "download_queue = MyQueue()\n",
    "resize_queue = MyQueue()\n",
    "upload_queue = MyQueue()\n",
    "done_queue = MyQueue()\n",
    "threads = [\n",
    "    Worker(download, download_queue, resize_queue),\n",
    "    Worker(resize, resize_queue, upload_queue),\n",
    "    Worker(upload, upload_queue, done_queue),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the worker functions vary in speeds, an earlier phase can prevent progress in later phases, backing up the pipeline. This causes later phases to starve and constantly check their input queues for new work in a tight loop. The outcome is that worker threads waste CPU time doing nothing useful (they’re constantly raising and catching IndexError exceptions).\n",
    "But that’s just the beginning of what’s wrong with this implementation. There are three more problems that you should also avoid. First, determining that all of the input work is complete requires yet another busy wait on the done_queue. Second, in Worker the run method will execute forever in its busy loop. There’s no way to signal to a worker thread that it’s time to exit.\n",
    "\n",
    "Third, and worst of all, a backup in the pipeline can cause the program to crash arbitrarily. If the first phase makes rapid progress but the second phase makes slow progress, then the queue connecting the first phase to the second phase will constantly increase in size. The second phase won’t be able to keep up. Given enough time and input data, the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the worker functions vary in speeds, an earlier phase can prevent progress in later phases, backing up the pipeline. This causes later phases to starve and constantly check their input queues for new work in a tight loop. The outcome is that worker threads waste CPU time doing nothing useful (they’re constantly raising and catching IndexError exceptions).\n",
    "But that’s just the beginning of what’s wrong with this implementation. There are three more problems that you should also avoid. First, determining that all of the input work is complete requires yet another busy wait on the done_queue. Second, in Worker the run method will execute forever in its busy loop. There’s no way to signal to a worker thread that it’s time to exit.\n",
    "\n",
    "Third, and worst of all, a backup in the pipeline can cause the program to crash arbitrarily. If the first phase makes rapid progress but the second phase makes slow progress, then the queue connecting the first phase to the second phase will constantly increase in size. The second phase won’t be able to keep up. Given enough time and input data, the program\n",
    "￼￼￼\n",
    "will eventually run out of memory and die.\n",
    "The lesson here isn’t that pipelines are bad; it’s that it’s hard to build a good producer- consumer queue yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Queue to the Rescue\n",
    "The Queue class from the queue built-in module provides all of the functionality you\n",
    "need to solve these problems.\n",
    "Queue eliminates the busy waiting in the worker by making the get method block until new data is available. For example, here I start a thread that waits for some input data on a queue:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer waiting\n",
      "Producer putting\n",
      "Consumer done\n",
      "Producer done\n"
     ]
    }
   ],
   "source": [
    "from queue import Queue\n",
    "queue = Queue()\n",
    "def consumer():\n",
    " \tprint('Consumer waiting')\n",
    " \tqueue.get()\n",
    " \tprint('Consumer done')\n",
    "\n",
    "# Runs after put() below\n",
    "thread = Thread(target=consumer)\n",
    "thread.start()\n",
    "\n",
    "print('Producer putting')\n",
    "queue.put(object())\n",
    "thread.join()\n",
    "print('Producer done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pipelines are a great way to organize sequences of work that run concurrently using multiple Python threads.\n",
    "* Be aware of the many problems in building concurrent pipelines: busy waiting, stopping workers, and memory explosion.\n",
    "* The Queue class has all of the facilities you need to build robust pipelines: blocking operations, buffer sizes, and joining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 40: Consider Coroutines to Run Many Functions Concurrently\n",
    "\n",
    "Threads give Python programmers a way to run multiple functions seemingly at the same time (see Item 37: “Use Threads for Blocking I/O, Avoid for Parallelism”). But there are three big problems with threads:\n",
    "\n",
    "* They require special tools to coordinate with each other safely (see Item 38: “Use Lock to Prevent Data Races in Threads” and Item 39: “Use Queue to Coordinate Work Between Threads”). \n",
    "\n",
    "* This makes code that uses threads harder to reason about than procedural, single-threaded code. This complexity makes threaded code more difficult to extend and maintain over time.\n",
    "\n",
    "* Threads require a lot of memory, about 8 MB per executing thread. On many computers, that amount of memory doesn’t matter for a dozen threads or so. But what if you want your program to run tens of thousands of functions “simultaneously”? These functions may correspond to user requests to a server, pixels on a screen, particles in a simulation, etc. Running a thread per unique activity just won’t work.\n",
    "\n",
    "* Threads are costly to start. If you want to constantly be creating new concurrent functions and finishing them, the overhead of using threads becomes large and slows\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼\n",
    "everything down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python can work around all these issues with coroutines. Coroutines let you have many seemingly simultaneous functions in your Python programs. They’re implemented as an extension to generators (see Item 16: “Consider Generators Instead of Returning Lists”). The cost of starting a generator coroutine is a function call. Once active, they each use less than 1 KB of memory until they’re exhausted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coroutines work by enabling the code consuming a generator to send a value back into the generator function after each yield expression. The generator function receives the value passed to the send function as the result of the corresponding yield expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received: First\n",
      "Received: Second\n"
     ]
    }
   ],
   "source": [
    "def my_coroutine():\n",
    "    while True:\n",
    "        received = yield\n",
    "        print('Received:', received)\n",
    "\n",
    "it = my_coroutine()\n",
    "next(it) # Prime the coroutine\n",
    "it.send('First')\n",
    "it.send('Second')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial call to next is required to prepare the generator for receiving the first send by advancing it to the first yield expression. Together, yield and send provide generators with a standard way to vary their next yielded value in response to external input.\n",
    "\n",
    "For example, say you want to implement a generator coroutine that yields the minimum value it’s been sent so far. Here, the bare yield prepares the coroutine with the initial minimum value sent in from the outside. Then the generator repeatedly yields the new minimum in exchange for the next value to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "4\n",
      "4\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def minimize():\n",
    "    current = yield\n",
    "    while True:\n",
    "        value = yield current\n",
    "        current = min(value, current)\n",
    "        \n",
    "it = minimize()\n",
    "next(it)\n",
    "print(it.send(10))\n",
    "print(it.send(4))\n",
    "print(it.send(22))\n",
    "print(it.send(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Coroutines provide an efficient way to run tens of thousands of functions seemingly at the same time.\n",
    "* Within a generator, the value of the yield expression will be whatever value was passed to the generator’s send method from the exterior code.\n",
    "* Coroutines give you a powerful tool for separating the core logic of your program from its interaction with the surrounding environment.\n",
    "* Python 2 doesn’t support yield from or returning values from generators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 41: Consider concurrent.futures for True Parallelism\n",
    "\n",
    "At some point in writing Python programs, you may hit the performance wall. Even after optimizing your code (see Item 58: “Profile Before Optimizing”), your program’s execution may still be too slow for your needs. On modern computers that have an increasing number of CPU cores, it’s reasonable to assume that one solution would be parallelism. What if you could split your code’s computation into independent pieces of work that run simultaneously across multiple CPU cores?\n",
    "Unfortunately, Python’s global interpreter lock (GIL) prevents true parallelism in threads (see Item 37: “Use Threads for Blocking I/O, Avoid for Parallelism”), so that option is out. Another common suggestion is to rewrite your most performance-critical code as an extension module using the C language. C gets you closer to the bare metal and can run faster than Python, eliminating the need for parallelism. C-extensions can also start native threads that run in parallel and utilize multiple CPU cores. Python’s API for C-extensions is well documented and a good choice for an escape hatch.\n",
    "\n",
    "But rewriting your code in C has a high cost. Code that is short and understandable in Python can become verbose and complicated in C. Such a port requires extensive testing to ensure that the functionality is equivalent to the original Python code and that no bugs have been introduced. Sometimes it’s worth it, which explains the large ecosystem of C- extension modules in the Python community that speed up things like text parsing, image compositing, and matrix math. There are even open source tools such as Cython (http://cython.org/) and Numba (http://numba.pydata.org/) that can ease the transition to C.\n",
    "\n",
    "The problem is that moving one piece of your program to C isn’t sufficient most of the time. Optimized Python programs usually don’t have one major source of slowness, but rather, there are often many significant contributors. To get the benefits of C’s bare metal and threads, you’d need to port large parts of your program, drastically increasing testing needs and risk. There must be a better way to preserve your investment in Python to solve difficult computational problems.\n",
    "\n",
    "The multiprocessing built-in module, easily accessed via the concurrent.futures built-in module, may be exactly what you need. It enables Python to utilize multiple CPU cores in parallel by running additional interpreters as child processes. These child processes are separate from the main interpreter, so their global\n",
    "￼￼￼￼￼￼￼￼￼￼\n",
    "interpreter locks are also separate. Each child can fully utilize one CPU core. Each child has a link to the main process where it receives instructions to do computation and returns results.\n",
    "For example, say you want to do something computationally intensive with Python and utilize multiple CPU cores. I’ll use an implementation of finding the greatest common divisor of two numbers as a proxy for a more computationally intense algorithm, like simulating fluid dynamics with the Navier-Stokes equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gcd(pair):\n",
    "       a, b = pair\n",
    "       low = min(a, b)\n",
    "       for i in range(low, 0, -1):\n",
    "           if a % i == 0 and b % i == 0:\n",
    "               return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcd((10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.402 seconds\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "numbers = [(1963309, 2265973), (2030677, 3814172),\n",
    "           (1551645, 2229620), (2039045, 2020802)]\n",
    "start = t.time()\n",
    "results = list(map(gcd, numbers))\n",
    "end = t.time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this code on multiple Python threads will yield no speed improvement because the GIL prevents Python from using multiple CPU cores in parallel. Here, I do the same computation as above using the concurrent.futures module with its ThreadPoolExecutor class and two worker threads (to match the number of CPU cores on my computer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1.488 seconds\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "start = t.time()\n",
    "pool = ThreadPoolExecutor(max_workers=2)\n",
    "results = list(pool.map(gcd, numbers))\n",
    "end = t.time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the surprising part: By changing a single line of code, something magical happens. If I replace the ThreadPoolExecutor with the ProcessPoolExecutor from the concurrent.futures module, everything speeds up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0.881 seconds\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "start = t.time()\n",
    "pool = ProcessPoolExecutor(max_workers=2)\n",
    "results = list(pool.map(gcd, numbers))\n",
    "end = t.time()\n",
    "print('Took %.3f seconds' % (end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running on my dual-core machine, it’s significantly faster! How is this possible? Here’s what the ProcessPoolExecutor class actually does (via the low-level constructs provided by the multiprocessing module):\n",
    "1. It takes each item from the numbers input data to map.\n",
    "2. It serializes it into binary data using the pickle module (see Item 44: “Makepickle Reliable with copyreg”).\n",
    "3. It copies the serialized data from the main interpreter process to a child interpreterprocess over a local socket.\n",
    "4. Next, it deserializes the data back into Python objects using pickle in the child process.\n",
    "5. It then imports the Python module containing the gcd function.\n",
    "6. It runs the function on the input data in parallel with other child processes. 7. It serializes the result back into bytes.\n",
    "8. It copies those bytes back through the socket.\n",
    "9. It deserializes the bytes back into Python objects in the parent process.\n",
    "10. Finally, it merges the results from multiple children into a single list to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it looks simple to the programmer, the multiprocessing module and ProcessPoolExecutor class do a huge amount of work to make parallelism possible. In most other languages, the only touch point you need to coordinate two threads is a single lock or atomic operation. The overhead of using multiprocessing is high because of all of the serialization and deserialization that must happen between the parent and child processes.\n",
    "This scheme is well suited to certain types of isolated, high-leverage tasks. By isolated, I mean functions that don’t need to share state with other parts of the program. By high- leverage, I mean situations in which only a small amount of data must be transferred between the parent and child processes to enable a large amount of computation. The greatest common denominator algorithm is one example of this, but many other mathematical algorithms work similarly.\n",
    "\n",
    "If your computation doesn’t have these characteristics, then the overhead of multiprocessing may prevent it from speeding up your program through parallelization. When that happens, multiprocessing provides more advanced facilities for shared memory, cross-process locks, queues, and proxies. But all of these features are very complex. It’s hard enough to reason about such tools in the memory space of a single process shared between Python threads. Extending that complexity to\n",
    "\n",
    "￼￼￼￼￼\n",
    "other processes and involving sockets makes this much more difficult to understand.\n",
    "I suggest avoiding all parts of multiprocessing and using these features via the simpler concurrent.futures module. You can start by using the ThreadPoolExecutor class to run isolated, high-leverage functions in threads. Later, you can move to the ProcessPoolExecutor to get a speedup. Finally, once you’ve completely exhausted the other options, you can consider using the multiprocessing module directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Moving CPU bottlenecks to C-extension modules can be an effective way to improve performance while maximizing your investment in Python code. However, the cost of doing so is high and may introduce bugs.\n",
    "* The multiprocessing module provides powerful tools that can parallelize certain types of Python computation with minimal effort.\n",
    "* The power of multiprocessing is best accessed through the concurrent.futures built-in module and its simple ProcessPoolExecutor class.\n",
    "* The advanced parts of the multiprocessing module should be avoided because they are so complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python takes a “batteries included” approach to the standard library. Many other languages ship with a small number of common packages and require you to look elsewhere for important functionality. Although Python also has an impressive repository of community- built modules, it strives to provide, in its default installation, the most important modules for common uses of the language.\n",
    "\n",
    "The full set of standard modules is too large to cover in this book. But some of these built- in packages are so closely intertwined with idiomatic Python that they may as well be part of the language specification. These essential built-in modules are especially important when writing the intricate, error-prone parts of programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 42: Define Function Decorators with functools.wraps\n",
    "* Decorators are Python syntax for allowing one function to modify another function at runtime.\n",
    "* Using decorators can cause strange behaviors in tools that do introspection, such as debuggers.\n",
    "* Use the wraps decorator from the functools built-in module when you define your own decorators to avoid any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trace(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print('%s(%r, %r) -> %r' %\n",
    "              (func.__name__, args, kwargs, result))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@trace\n",
    "def fibonacci(n):\n",
    "    if n in (0, 1):\n",
    "        return n\n",
    "    return (fibonacci(n - 2) + fibonacci(n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fibonacci((1,), {}) -> 1\n",
      "fibonacci((0,), {}) -> 0\n",
      "fibonacci((1,), {}) -> 1\n",
      "fibonacci((2,), {}) -> 1\n",
      "fibonacci((3,), {}) -> 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fibonacci(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function wrapper in module __main__:\n",
      "\n",
      "wrapper(*args, **kwargs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "def trace(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        print('%s(%r, %r) -> %r' %\n",
    "              (func.__name__, args, kwargs, result))\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@trace\n",
    "def fibonacci(n):\n",
    "    if n in (0, 1):\n",
    "        return n\n",
    "    return (fibonacci(n - 2) + fibonacci(n - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fibonacci((1,), {}) -> 1\n",
      "fibonacci((0,), {}) -> 0\n",
      "fibonacci((1,), {}) -> 1\n",
      "fibonacci((2,), {}) -> 1\n",
      "fibonacci((3,), {}) -> 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fibonacci(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fibonacci in module __main__:\n",
      "\n",
      "fibonacci(n)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fibonacci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 43: Consider contextlib and with Statements for Reusable try/finally Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The with statement in Python is used to indicate when code is running in a special context. For example, mutual exclusion locks (see Item 38: “Use Lock to Prevent Data Races in Threads”) can be used in with statements to indicate that the indented code only runs while the lock is held."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "lock = Lock()\n",
    "with lock:\n",
    "print('Lock is held')\n",
    "\n",
    "The example above is equivalent to this try/finally construction because the Lock\n",
    "class properly enables the with statement.\n",
    "\n",
    "=======\n",
    "lock.acquire()\n",
    "try:\n",
    "    print('Lock is held')\n",
    "finally:\n",
    "    lock.release()\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The with statement version of this is better because it eliminates the need to write the repetitive code of the try/finally construction. It’s easy to make your objects and functions capable of use in with statements by using the contextlib built-in module. \n",
    "\n",
    "This module contains the contextmanager decorator, which lets a simple function be used in with statements. This is much easier than defining a new class with the special methods __enter__ and __exit__ (the standard way).\n",
    "\n",
    "For example, say you want a region of your code to have more debug logging sometimes. Here, I define a function that does logging at two severity levels:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The with statement allows you to reuse logic from try/finally blocks and reduce visual noise.\n",
    "￼￼￼￼￼\n",
    "* The contextlib built-in module provides a contextmanager decorator that makes it easy to use your own functions in with statements.\n",
    "* The value yielded by context managers is supplied to the as part of the with statement. It’s useful for letting your code directly access the cause of the special context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "@contextmanager\n",
    "def log_level(level, name):\n",
    "       logger = logging.getLogger(name)\n",
    "       old_level = logger.getEffectiveLevel()\n",
    "       logger.setLevel(level)\n",
    "       try:\n",
    "           yield logger\n",
    "       finally:\n",
    "           logger.setLevel(old_level)\n",
    "            \n",
    "import logging\n",
    "with log_level(logging.DEBUG, 'my-log') as logger:\n",
    "    logger.debug('This is my message!')\n",
    "    logging.debug('This will not print')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yield expression is the point at which the with block’s contents will execute. Any exceptions that happen in the with block will be re-raised by the yield expression for you to catch in the helper function (see Item 40: “Consider Coroutines to Run Many Functions Concurrently” for an explanation of how that works).\n",
    "\n",
    "Now, I can call the same logging function again, but in the debug_logging context. This time, all of the debug messages are printed to the screen during the with block. The same function running outside the with block won’t print debug messages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 44: Make pickle Reliable with copyreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pickle built-in module can serialize Python objects into a stream of bytes and deserialize bytes back into objects. Pickled byte streams shouldn’t be used to communicate between untrusted parties. The purpose of pickle is to let you pass Python objects between programs that you control over binary channels.\n",
    "\n",
    "The pickle module’s serialization format is unsafe by design. The serialized data contains what is essentially a program that describes how to reconstruct the original Python object. This means a malicious pickle payload could be used to compromise any part of the Python program that attempts to deserialize it.\n",
    "\n",
    "In contrast, the json module is safe by design. Serialized JSON data contains a simple description of an object hierarchy. Deserializing JSON data does not expose a Python program to any additional risk. Formats like JSON should be used for communication between programs or people that don’t trust each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 1, 'lives': 3}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "class GameState(object):\n",
    "    def __init__(self):\n",
    "        self.level = 0\n",
    "        self.lives = 4\n",
    "\n",
    "state = GameState()\n",
    "state.level += 1  # Player beat a level\n",
    "state.lives -= 1  # Player had to try again\n",
    "\n",
    "# When the user quits playing, the program can save the state of the game to a file so it \n",
    "# can be resumed at a later time. The pickle module makes it easy to do this. Here, \n",
    "# I dump the GameState object directly to a file:\n",
    "state_path = '/tmp/game_state.bin'\n",
    "with open(state_path, 'wb') as f:\n",
    "    pickle.dump(state, f)\n",
    "\n",
    "#Later, I can load the file and get back the GameState object as if it had never been\n",
    "# serialized.\n",
    "\n",
    "with open(state_path, 'rb') as f:\n",
    "       state_after = pickle.load(f)\n",
    "       print(state_after.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach is what happens as the game’s features expand over time. Imagine you want the player to earn points towards a high score. To track the player’s points, you’d add a new field to the GameState class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 0, 'points': 0, 'lives': 4}\n",
      "{'level': 1, 'lives': 3}\n"
     ]
    }
   ],
   "source": [
    "class GameState(object):\n",
    "    def __init__(self):\n",
    "        self.level = 0\n",
    "        self.lives = 4\n",
    "        self.points = 0\n",
    "\n",
    "state = GameState()\n",
    "serialized = pickle.dumps(state)\n",
    "state_after = pickle.loads(serialized)\n",
    "print(state_after.__dict__)\n",
    "    \n",
    "with open(state_path, 'rb') as f:\n",
    "       state_after = pickle.load(f)\n",
    "       print(state_after.__dict__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The points attribute is missing! This is especially confusing because the returned object is an instance of the new GameState class.\n",
    "\n",
    "This behavior is a byproduct of the way the pickle module works. Its primary use case is making it easy to serialize objects. As soon as your use of pickle expands beyond trivial usage, the module’s functionality starts to break down in surprising ways.\n",
    "\n",
    "Fixing these problems is straightforward using the copyreg built-in module. The copyreg module lets you register the functions responsible for serializing Python objects, allowing you to control the behavior of pickle and make it more reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'level': 0, 'points': 1000, 'lives': 4}\n"
     ]
    }
   ],
   "source": [
    "import copyreg\n",
    "class GameState(object):\n",
    "    def __init__(self, level=0, lives=4, points=0):\n",
    "        self.level = level\n",
    "        self.lives = lives\n",
    "        self.points = points\n",
    "\n",
    "def pickle_game_state(game_state):\n",
    "    kwargs = game_state.__dict__\n",
    "    return unpickle_game_state, (kwargs,)\n",
    "\n",
    "def unpickle_game_state(kwargs):\n",
    "    return GameState(**kwargs)\n",
    "\n",
    "copyreg.pickle(GameState, pickle_game_state)\n",
    "\n",
    "\n",
    "state = GameState()\n",
    "state.points += 1000\n",
    "serialized = pickle.dumps(state)\n",
    "state_after = pickle.loads(serialized)\n",
    "print(state_after.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The pickle built-in module is only useful for serializing and deserializing objects between trusted programs.\n",
    "* The pickle module may break down when used for more than trivial use cases.\n",
    "* Use the copyreg built-in module with pickle to add missing attribute values, allow versioning of classes, and provide stable import paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 45: Use datetime Instead of time for Local Clocks\n",
    "\n",
    "Coordinated Universal Time (UTC) is the standard, time-zone-independent representation of time. UTC works great for computers that represent time as seconds since the UNIX epoch. But UTC isn’t ideal for humans. Humans reference time relative to where they’re currently located. People say “noon” or “8 am” instead of “UTC 15:00 minus 7 hours.” If your program handles time, you’ll probably find yourself converting time between UTC and local clocks to make it easier for humans to understand.\n",
    "\n",
    "Python provides two ways of accomplishing time zone conversions. The old way, using the time built-in module, is disastrously error prone. The new way, using the datetime built-in module, works great with some help from the community-built package named pytz.\n",
    "You should be acquainted with both time and datetime to thoroughly understand why datetime is the best choice and time should be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is the platform-dependent nature of the time module. Its actual behavior is determined by how the underlying C functions work with the host operating system. This makes the functionality of the time module unreliable in Python. The time module fails to consistently work properly for multiple local times. Thus, you should avoid the time module for this purpose. If you must use time, only use it to convert between UTC and the host computer’s local time. For all other types of conversions, use the datetime module.\n",
    "\n",
    "Unlike the time module, the datetime module has facilities for reliably converting from one local time to another local time. However, datetime only provides the machinery for time zone operations with its tzinfo class and related methods. What’s missing are the time zone definitions besides UTC.\n",
    "Luckily, the Python community has addressed this gap with the pytz module that’s available for download from the Python Package Index (https://pypi.python.org/pypi/pytz/). pytz contains a full database of every time zone definition you might need.\n",
    "To use pytz effectively, you should always convert local times to UTC first. Perform any\n",
    "￼￼￼\n",
    "datetime operations you need on the UTC values (such as offsetting). Then, convert to local times as a final step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avoid using the time module for translating between different time zones.\n",
    "* Use the datetime built-in module along with the pytz module to reliably convert between times in different time zones.\n",
    "* Always represent time in UTC and do conversions to local time as the final step before presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 46: Use Built-in Algorithms and Data Structures\n",
    "\n",
    "When you’re implementing Python programs that handle a non-trivial amount of data, you’ll eventually see slowdowns caused by the algorithmic complexity of your code. This usually isn’t the result of Python’s speed as a language (see Item 41: “Consider concurrent.futures for True Parallelism” if it is). The issue, more likely, is that you aren’t using the best algorithms and data structures for your problem.\n",
    "\n",
    "Luckily, the Python standard library has many of the algorithms and data structures you’ll need to use built in. Besides speed, using these common algorithms and data structures can make your life easier. Some of the most valuable tools you may want to use are tricky to implement correctly. Avoiding reimplementation of common functionality will save you time and headaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double-ended Queue\n",
    "The deque class from the collections module is a double-ended queue. It provides constant time operations for inserting or removing items from its beginning or end. This makes it ideal for first-in-first-out (FIFO) queues.\n",
    "Click here to view code image\n",
    "\n",
    "The list built-in type also contains an ordered sequence of items like a queue. You can insert or remove items from the end of a list in constant time. But inserting or removing items from the head of a list takes linear time, which is much slower than the constant time of a deque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "fifo = deque()\n",
    "fifo.append(1)      # Producer\n",
    "fifo.append(11)\n",
    "print(fifo.popleft())  # Consumer\n",
    "print(fifo.popleft())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Ordered Dictionary\n",
    "Standard dictionaries are unordered. That means a dict with the same keys and values can result in different orders of iteration. This behavior is a surprising byproduct of the way the dictionary’s fast hash table is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The OrderedDict class from the collections module is a special type of dictionary that keeps track of the order in which its keys were inserted. Iterating the keys of an OrderedDict has predictable behavior. This can vastly simplify testing and debugging by making all code deterministic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 red\n",
      "2 blue\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "a = OrderedDict()\n",
    "a['foo'] = 1\n",
    "a['bar'] = 2\n",
    "b = OrderedDict()\n",
    "b['foo'] = 'red'\n",
    "b['bar'] = 'blue'\n",
    "for value1, value2 in zip(a.values(), b.values()):\n",
    "    print(value1, value2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Dictionary\n",
    "Dictionaries are useful for bookkeeping and tracking statistics. One problem with dictionaries is that you can’t assume any keys are already present. That makes it clumsy to do simple things like increment a counter stored in a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "stats = {}\n",
    "key = 'my_counter'\n",
    "if key not in stats:\n",
    "   stats[key] = 0\n",
    "stats[key] += 1\n",
    "\n",
    "print(stats['my_counter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defaultdict class from the collections module simplifies this by automatically storing a default value when a key doesn’t exist. All you have to do is provide a function that will return the default value each time a key is missing. In this example, the int built-in function returns 0 (see Item 23: “Accept Functions for Simple Interfaces Instead of Classes” for another example). Now, incrementing a counter is simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "stats = defaultdict(int)\n",
    "stats['my_counter'] += 1\n",
    "print(stats['my_counter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heap Queue\n",
    "\n",
    "Heaps are useful data structures for maintaining a priority queue. The heapq module provides functions for creating heaps in standard list types with functions like heappush, heappop, and nsmallest.\n",
    "\n",
    "Items of any priority can be inserted into the heap in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n"
     ]
    }
   ],
   "source": [
    "from heapq import heappush, heappop, nsmallest\n",
    "a = []\n",
    "heappush(a, 5)\n",
    "heappush(a, 3)\n",
    "heappush(a, 7)\n",
    "heappush(a, 4)\n",
    "\n",
    "print(heappop(a), heappop(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting list is easy to use outside of heapq. Accessing the 0 index of the heap will always return the smallest item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert a[0] == nsmallest(1, a)[0] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [1, 5, 422, 7]\n",
      "After:  [1, 5, 7, 422]\n"
     ]
    }
   ],
   "source": [
    "heappush(a, 422)\n",
    "heappush(a, 1)\n",
    "print('Before:', a)\n",
    "a.sort()\n",
    "print('After: ', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Each of these heapq operations takes logarithmic time in proportion to the length of the list. Doing the same work with a standard Python list would scale linearly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for an item in a list takes linear time proportional to its length when you call the index method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = list(range(10**6))\n",
    "i = x.index(991234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bisect import bisect_left\n",
    "i = bisect_left(x, 991234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterator Tools\n",
    "The itertools built-in module contains a large number of functions that are useful for organizing and interacting with iterators (see Item 16: “Consider Generators Instead of Returning Lists” and Item 17: “Be Defensive When Iterating Over Arguments” for background). Not all of these are available in Python 2, but they can easily be built using simple recipes documented in the module. See help(itertools) in an interactive Python session for more details.\n",
    "\n",
    "The itertools functions fall into three main categories: Linking iterators together\n",
    "•  chain: Combines multiple iterators into a single sequential iterator.\n",
    "• cycle: Repeats an iterator’s items forever.\n",
    "• tee: Splits a single iterator into multiple parallel iterators.\n",
    "• zip_longest: A variant of the zip built-in function that works well with iterators of different lengths.\n",
    "Filtering items from an iterator\n",
    "• islice: Slices an iterator by numerical indexes without copying.\n",
    "• takewhile: Returns items from an iterator while a predicate function returns True.\n",
    "• dropwhile: Returns items from an iterator once the predicate function returns False for the first time.\n",
    "• filterfalse: Returns all items from an iterator where a predicate function returns False. The opposite of the filter built-in function.\n",
    "Combinations of items from iterators\n",
    "• product: Returns the Cartesian product of items from an iterator, which is a nice alternative to deeply nested list comprehensions.\n",
    "• permutations: Returns ordered permutations of length N with items from an iterator.\n",
    "• combination: Returns the unordered combinations of length N with unrepeated items from an iterator.\n",
    "There are even more functions and recipes available in the itertools module that I don’t mention here. Whenever you find yourself dealing with some tricky iteration code, it’s worth looking at the itertools documentation again to see whether there’s anything there for you to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 47: Use decimal When Precision Is Paramount\n",
    "\n",
    "Python is an excellent language for writing code that interacts with numerical data. Python’s integer type can represent values of any practical size. Its double-precision floating point type complies with the IEEE 754 standard. The language also provides a standard complex number type for imaginary values. However, these aren’t enough for every situation.\n",
    "\n",
    "For example, say you want to compute the amount to charge a customer for an international phone call. You know the time in minutes and seconds that the customer was on the phone (say, 3 minutes 42 seconds). You also have a set rate for the cost of calling Antarctica from the United States ($1.45/minute). What should the charge be?\n",
    "With floating point math, the computed charge seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rate = 1.45\n",
    "seconds = 3*60 + 42\n",
    "cost = rate * seconds / 60\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal, ROUND_UP\n",
    "rate = Decimal('0.05')\n",
    "seconds = Decimal('5')\n",
    "cost = rate * seconds / Decimal('60')\n",
    "print(cost)\n",
    "rounded = cost.quantize(Decimal('0.01'), rounding=ROUND_UP)\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 48: Know Where to Find Community-Built Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a central repository of modules (https://pypi.python.org) for you to install and use in your programs. These modules are built and maintained by people like you: the Python community. When you find yourself facing an unfamiliar challenge, the Python Package Index (PyPI) is a great place to look for code that will get you closer to your goal.\n",
    "\n",
    "To use the Package Index, you’ll need to use a command-line tool named pip. pip is installed by default in Python 3.4 and above (it’s also accessible with python -m pip). For earlier versions, you can find instructions for installing pip on the Python Packaging website (https://packaging.python.org).\n",
    "\n",
    "Once installed, using pip to install a new module is simple. For example, here I install the pytz module that I used in another item in this chapter (see Item 45: “Use datetime Instead of time for Local Clocks”):\n",
    "\n",
    "In the example above, I used the pip3 command-line to install the Python 3 version of the package. The pip command-line (without the 3) is also available for installing packages for Python 2. The majority of popular packages are now available for either version of Python (see Item 1: “Know Which Version of Python You’re Using”). pip can also be used with pyvenv to track sets of packages to install for your projects (see Item 53: “Use Virtual Environments for Isolated and Reproducible Dependencies”)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Python Package Index (PyPI) contains a wealth of common packages that are built and maintained by the Python community.\n",
    "* pip is the command-line tool to use for installing packages from PyPI.\n",
    "* pip is installed by default in Python 3.4 and above; you must install it yourself for older versions.\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼\n",
    "* The majority of PyPI modules are free and open source software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are language features in Python to help you construct well-defined APIs with clear interface boundaries. The Python community has established best practices that maximize the maintainability of code over time. There are also standard tools that ship with Python that enable large teams to work together across disparate environments.\n",
    "\n",
    "Collaborating with others on Python programs requires being deliberate about how you write your code. Even if you’re working on your own, chances are you’ll be using code written by someone else via the standard library or open source packages. It’s important to understand the mechanisms that make it easy to collaborate with other Python programmers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 49: Write Docstrings for Every Function, Class, and Module\n",
    "\n",
    "Documentation in Python is extremely important because of the dynamic nature of the language. Python provides built-in support for attaching documentation to blocks of code. Unlike many other languages, the documentation from a program’s source code is directly accessible as the program runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accessibility of documentation makes interactive development easier. You can inspect functions, classes, and modules to see their documentation by using the help built-in function. This makes the Python interactive interpreter (the Python “shell”) and tools like IPython Notebook (http://ipython.org) a joy to use while you’re developing algorithms, testing APIs, and writing code snippets.\n",
    "\n",
    "A standard way of defining documentation makes it easy to build tools that convert the text into more appealing formats (like HTML). This has led to excellent\n",
    "￼￼￼￼￼\n",
    "documentation-generation tools for the Python community, such as Sphinx (http://sphinx-doc.org). It’s also enabled community-funded sites like Read the Docs (https://readthedocs.org) that provide free hosting of beautiful-looking documentation for open source Python projects.\n",
    "\n",
    "Python’s first-class, accessible, and good-looking documentation encourages people to write more documentation. The members of the Python community have a strong belief in the importance of documentation. There’s an assumption that “good code” also means well-documented code. This means that you can expect most open source Python libraries to have decent documentation.\n",
    "To participate in this excellent culture of documentation, you need to follow a few guidelines when you write docstrings. The full details are discussed online in PEP 257 (http://www.python.org/dev/peps/pep-0257/). There are a few best-practices you should be sure to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def palindrome(word):\n",
    "    \"\"\"Return True if the given word is a palindrome.\"\"\"\n",
    "    return word == word[::-1]\n",
    "print(repr(palindrome.__doc__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documenting Modules\n",
    "\n",
    "Each module should have a top-level docstring. This is a string literal that is the first statement in a source file. It should use three double quotes (\"\"\"). The goal of this docstring is to introduce the module and its contents.\n",
    "\n",
    "The first line of the docstring should be a single sentence describing the module’s purpose. The paragraphs that follow should contain the details that all users of the module should know about its operation. The module docstring is also a jumping-off point where you can highlight important classes and functions found in the module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write documentation for every module, class, and function using docstrings. Keep them up to date as your code changes.\n",
    "* For modules: Introduce the contents of the module and any important classes or functions all users should know about.\n",
    "* For classes: Document behavior, important attributes, and subclass behavior in the docstring following the class statement.\n",
    "* For functions and methods: Document every argument, returned value, raised exception, and other behaviors in the docstring following the def statemen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 50: Use Packages to Organize Modules and Provide Stable APIs\n",
    "\n",
    "As the size of a program’s codebase grows, it’s natural for you to reorganize its structure. You split larger functions into smaller functions. You refactor data structures into helper classes (see Item 22: “Prefer Helper Classes Over Bookkeeping with Dictionaries and Tuples”). You separate functionality into various modules that depend on each other.\n",
    "\n",
    "At some point, you’ll find yourself with so many modules that you need another layer in your program to make it understandable. For this purpose, Python provides packages. Packages are modules that contain other modules.\n",
    "\n",
    "In most cases, packages are defined by putting an empty file named __init__.py into a directory. Once __init__.py is present, any other Python files in that directory will be available for import using a path relative to the directory. For example, imagine that you have the following directory structure in your program.\n",
    "```\n",
    "main.py\n",
    "   mypackage/__init__.py\n",
    "   mypackage/models.py\n",
    "   mypackage/utils.py\n",
    "To import the utils module, you use the absolute module name that includes the package directory’s name.\n",
    "# main.py\n",
    "from mypackage import utils\n",
    "```\n",
    "This pattern continues when you have package directories present within other packages\n",
    "(like mypackage.foo.bar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stable APIs\n",
    "The second use of packages in Python is to provide strict, stable APIs for external consumers.\n",
    "When you’re writing an API for wider consumption, like an open source package (see Item 48: “Know Where to Find Community-Built Modules”), you’ll want to provide stable functionality that doesn’t change between releases. To ensure that happens, it’s important to hide your internal code organization from external users. This enables you to refactor and improve your package’s internal modules without breaking existing users.\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼\n",
    "Python can limit the surface area exposed to API consumers by using the __all__ special attribute of a module or package. The value of __all__ is a list of every name to export from the module as part of its public API. When consuming code does from foo import *, only the attributes in foo.__all__ will be imported from foo. If __all__ isn’t present in foo, then only public attributes, those without a leading underscore, are imported (see Item 27: “Prefer Public Attributes Over Private Ones”).\n",
    "For example, say you want to provide a package for calculating collisions between moving projectiles. Here, I define the models module of mypackage to contain the representation of projectiles:\n",
    "Click here to view code image\n",
    " ```  # models.py\n",
    "   __all__ = [‘Projectile’]\n",
    "   class Projectile(object):\n",
    "       def __init__(self, mass, velocity):\n",
    "           self.mass = mass\n",
    "           self.velocity = velocity\n",
    "```\n",
    "           \n",
    "I also define a utils module in mypackage to perform operations on the Projectile instances, such as simulating collisions between them.\n",
    "Click here to view code image\n",
    "```# utils.py\n",
    "   from . models import Projectile\n",
    "   __all__ = [‘simulate_collision’]\n",
    "def _dot_product(a, b): #...\n",
    "def simulate_collision(a, b): #...\n",
    "```\n",
    "Now, I’d like to provide all of the public parts of this API as a set of attributes that are available on the mypackage module. This will allow downstream consumers to always import directly from mypackage instead of importing from mypackage.models or mypackage.utils. This ensures that the API consumer’s code will continue to work even if the internal organization of mypackage changes (e.g., models.py is deleted).\n",
    "To do this with Python packages, you need to modify the __init__.py file in the mypackage directory. This file actually becomes the contents of the mypackage module when it’s imported. Thus, you can specify an explicit API for mypackage by limiting what you import into __init__.py. Since all of my internal modules already specify __all__, I can expose the public interface of mypackage by simply importing everything from the internal modules and updating __all__ accordingly.\n",
    "  ``` # __init__.py\n",
    "   __all__ = []\n",
    "   from . models import *\n",
    "   __all__ += models.__all__\n",
    "   from . utils import *\n",
    "   __all__ += utils.__all__ ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beware of import *\n",
    "Import statements like from x import y are clear because the source of y is explicitly the x package or module. Wildcard imports like from foo import * can also be useful, especially in interactive Python sessions. However, wildcards make code more difficult to understand.\n",
    "\n",
    "from foo import * hides the source of names from new readers of the code. If a module has multiple import * statements, you’ll need to check all of the referenced modules to figure out where a name was defined.\n",
    "\n",
    "Names from import * statements will overwrite any conflicting names within the containing module. This can lead to strange bugs caused by accidental interactions between your code and overlapping names from multiple import * statements.\n",
    "\n",
    "The safest approach is to avoid import * in your code and explicitly import names with the from x import y style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages in Python are modules that contain other modules. Packages allow you to organize your code into separate, non-conflicting namespaces with unique absolute module names.\n",
    "\n",
    "Simple packages are defined by adding an __init__.py file to a directory that contains other source files. These files become the child modules of the directory’s package. Package directories may also contain other packages.\n",
    "\n",
    "You can provide an explicit API for a module by listing its publicly visible names in\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼￼￼\n",
    "its __all__ special attribute.\n",
    "\n",
    "You can hide a package’s internal implementation by only importing public names in the package’s __init__.py file or by naming internal-only members with a leading underscore.\n",
    "\n",
    "When collaborating within a single team or on a single codebase, using __all__ for explicit APIs is probably unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 51: Define a Root Exception to Insulate Callers from APIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defining root exceptions for your modules allows API consumers to insulate themselves from your API.\n",
    "* Catching root exceptions can help you find bugs in code that consumes an API.\n",
    "* Catching the Python Exception base class can help you find bugs in API implementations.\n",
    "* Intermediate root exceptions let you add more specific types of exceptions in the future without breaking your API consumers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 52: Know How to Break Circular Dependencies\n",
    "\n",
    "Inevitably, while you’re collaborating with others, you’ll find a mutual interdependency between modules. It can even happen while you work by yourself on the various parts of a single program.\n",
    "\n",
    "\n",
    "#### what happen when you import ?\n",
    "\n",
    "To understand what’s happening here, you need to know the details of Python’s import machinery. When a module is imported, here’s what Python actually does in depth-first order:\n",
    "\n",
    "1. Searches for your module in locations from sys.path\n",
    "2. Loads the code from the module and ensures that it compiles\n",
    "3. Creates a corresponding empty module object\n",
    "4. Inserts the module into sys.module\n",
    "5. Runs the code in the module object to define its contents\n",
    "\n",
    "```\n",
    "# app.py\n",
    "import dialog\n",
    "class Prefs(object):\n",
    "\tdef get(self, name):\n",
    "\t\tprint(\"Prefs->get\")\n",
    "\n",
    "\n",
    "prefs = Prefs()\n",
    "dialog.show()\n",
    "\n",
    "# dialog.py\n",
    "import app\n",
    "class Dialog(object):\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "save_dialog = Dialog(app.prefs.get('save_dir'))\n",
    "\n",
    "def show():\n",
    "   print(\"show\")\n",
    "\n",
    "\n",
    "#main.py\n",
    "import app\n",
    "\n",
    "Traceback (most recent call last):\n",
    "     File “main.py”, line 4, in <module>\n",
    "       import app\n",
    "     File “app.py”, line 4, in <module>\n",
    "       import dialog\n",
    "     File “dialog.py”, line 16, in <module>\n",
    "       save_dialog = Dialog(app.prefs.get(‘save_dir’))\n",
    "   AttributeError: ‘module’ object has no attribute ‘prefs’\n",
    "   \n",
    "```\n",
    "\n",
    "The problem with a circular dependency is that the attributes of a module aren’t defined until the code for those attributes has executed (after step #5). But the module can be loaded with the import statement immediately after it’s inserted into sys.modules (after step #4).\n",
    "\n",
    "In the example above, the app module imports dialog before defining anything. Then, the dialog module imports app. Since app still hasn’t finished running—it’s currently importing dialog—the app module is just an empty shell (from step #4). The AttributeError is raised (during step #5 for dialog) because the code that defines prefs hasn’t run yet (step #5 for app isn’t complete).\n",
    "\n",
    "#### Solution 1 - bottom of the dependency tree\n",
    "The best solution to this problem is to refactor your code so that the prefs data structure is at the bottom of the dependency tree. Then, both app and dialog can import the same utility module and avoid any circular dependencies. But such a clear division isn’t always possible or could require too much refactoring to be worth the effort.\n",
    "There are three other ways to break circular dependencies.\n",
    "Reordering Imports\n",
    "\n",
    "#### Sol 2 - Reordering Imports\n",
    "\n",
    "The first approach is to change the order of imports. For example, if you import the dialog module toward the bottom of the app module, after its contents have run, the AttributeError goes away.\n",
    "```\n",
    "# app.py\n",
    "class Prefs(object): #...\n",
    "   prefs = Prefs()\n",
    "   import dialog  # Moved\n",
    "   dialog.show()\n",
    "```\n",
    "This works because, when the dialog module is loaded late, its recursive import of app will find that app.prefs has already been defined (step #5 is mostly done for app).\n",
    "\n",
    "Although this avoids the AttributeError, it goes against the PEP 8 style guide (see\n",
    "Item 2: “Follow the PEP 8 Style Guide”). The style guide suggests that you always put imports at the top of your Python files. This makes your module’s dependencies clear to new readers of the code. It also ensures that any module you depend on is in scope and available to all the code in your module.\n",
    "\n",
    "Having imports later in a file can be brittle and can cause small changes in the ordering of your code to break the module entirely. Thus, you should avoid import reordering to solve your circular dependency issues.\n",
    "\n",
    "#### Sol 3 - Import, Configure, Run\n",
    "\n",
    "A second solution to the circular imports problem is to have your modules minimize side effects at import time. You have your modules only define functions, classes, and constants.\n",
    "\n",
    "You avoid actually running any functions at import time. Then, you have each module provide a configure function that you call once all other modules have finished importing. The purpose of configure is to prepare each module’s state by accessing the attributes of other modules.\n",
    "\n",
    "You run configure after all modules have been imported (step #5 is complete), so all attributes must be defined.\n",
    "\n",
    "Here, I redefine the dialog module to only access the prefs object when configure is called:\n",
    "```\n",
    "Click here to view code image\n",
    "# dialog.py\n",
    "   import app\n",
    "   class Dialog(object):\n",
    "#...\n",
    "   save_dialog = Dialog()\n",
    "def show(): #...\n",
    "   def configure():\n",
    "       save_dialog.save_dir = app.prefs.get(‘save_dir’)\n",
    "I also redefine the app module to not run any activities on import.\n",
    "# app.py\n",
    "   import dialog\n",
    "   class Prefs(object):\n",
    "#...\n",
    "prefs = Prefs()\n",
    "def configure(): #...\n",
    "```\n",
    "Finally, the main module has three distinct phases of execution: import everything, configure everything, and run the first activity.\n",
    "```\n",
    "# main.py\n",
    "   import app\n",
    "   import dialog\n",
    "   app.configure()\n",
    "   dialog.configure()\n",
    "   dialog.show()\n",
    "   ```\n",
    "This works well in many situations and enables patterns like dependency injection. But sometimes it can be difficult to structure your code so that an explicit configure step is possible. Having two distinct phases within a module can also make your code harder to\n",
    "￼\n",
    "read because it separates the definition of objects from their configuration.\n",
    "```\n",
    "####  Sol 4- Dynamic Import\n",
    "\n",
    "The third—and often simplest—solution to the circular imports problem is to use an import statement within a function or method. This is called a dynamic import because the module import happens while the program is running, not while the program is first starting up and initializing its modules.\n",
    "\n",
    "Here, I redefine the dialog module to use a dynamic import. The dialog.show function imports the app module at runtime instead of the dialog module importing app at initialization time.\n",
    "\n",
    "```\n",
    "Click here to view code image\n",
    "-- dialog.py\n",
    "class Dialog(object): #...\n",
    "   save_dialog = Dialog()\n",
    "def show():\n",
    "import app # Dynamic import \n",
    "save_dialog.save_dir = app.prefs.get(‘save_dir’) \n",
    "The app module can now be the same as it was in the original example. It imports dialog at the top and calls dialog.show at the bottom.\n",
    "\n",
    "--app.py\n",
    "\n",
    "   import dialog\n",
    "class Prefs(object): #...\n",
    "   prefs = Prefs()\n",
    "   dialog.show()\n",
    "   \n",
    "```\n",
    "\n",
    "This approach has a similar effect to the import, configure, and run steps from before. The difference is that this requires no structural changes to the way the modules are defined and imported. You’re simply delaying the circular import until the moment you must access the other module. At that point, you can be pretty sure that all other modules have already been initialized (step #5 is complete for everything).\n",
    "\n",
    "In general, it’s good to avoid dynamic imports like this. The cost of the import statement is not negligible and can be especially bad in tight loops. By delaying execution, dynamic imports also set you up for surprising failures at runtime, such as SyntaxError exceptions long after your program has started running (see Item 56: “Test Everything with unittest” for how to avoid that). However, these downsides are often better than the alternative of restructuring your entire program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 53: Use Virtual Environments for Isolated and Reproducible Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building larger and more complex programs often leads you to rely on various packages from the Python community (see Item 48: “Know Where to Find Community-Built Modules”). You’ll find yourself running pip to install packages like pytz, numpy, and many others.\n",
    "\n",
    "The problem is that, by default, pip installs new packages in a global location. That causes all Python programs on your system to be affected by these installed modules. In theory, this shouldn’t be an issue. If you install a package and never import it, how could it affect your programs?\n",
    "\n",
    "The trouble comes from transitive dependencies: the packages that the packages you install depend on. For example, you can see what the Sphinx package depends on after installing it by asking pip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New versions of a library can subtly change behaviors that API- consuming code relies on. Users on a system may upgrade one package to a new version but not others, which could dependencies. There’s a constant risk of the ground moving beneath your feet.\n",
    "\n",
    "These difficulties are magnified when you collaborate with other developers who do their work on separate computers. It’s reasonable to assume that the versions of Python and global packages they have installed on their machines will be slightly different than your own. This can cause frustrating situations where a codebase works perfectly on one programmer’s machine and is completely broken on another’s.\n",
    "\n",
    "The solution to all of these problems is a tool called pyvenv, which provides virtual environments. Since Python 3.4, the pyvenv command-line tool is available by default along with the Python installation (it’s also accessible with python -m venv). Prior versions of Python require installing a separate package (with pip install virtualenv) and using a command-line tool called virtualenv.\n",
    "\n",
    "pyvenv allows you to create isolated versions of the Python environment. Using pyvenv, you can have many different versions of the same package installed on the same system at the same time without conflicts. This lets you work on many different projects and use many different tools on the same computer.\n",
    "pyvenv does this by installing explicit versions of packages and their dependencies into completely separate directory structures. This makes it possible to reproduce a Python environment that you know will work with your code. It’s a reliable way to avoid surprising breakages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Virtual environments allow you to use pip to install many different versions of the same package on the same machine without conflicts.\n",
    "* Virtual environments are created with pyvenv, enabled with source bin/activate, and disabled with deactivate.\n",
    "* You can dump all of the requirements of an environment with pip freeze. You can reproduce the environment by supplying the requirements.txt file to pip install -r.\n",
    "* In versions of Python before 3.4, the pyvenv tool must be downloaded and installed separately. The command-line tool is called virtualenv instead of pyvenv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 54: Consider Module-Scoped Code to Configure Deployment Environments\n",
    "\n",
    "A deployment environment is a configuration in which your program runs. Every program has at least one deployment environment, the production environment. The goal of writing a program in the first place is to put it to work in the production environment and achieve some kind of outcome.\n",
    "\n",
    "Writing or modifying a program requires being able to run it on the computer you use for developing. The configuration of your development environment may be much different from your production environment. For example, you may be writing a program for supercomputers using a Linux workstation.\n",
    "\n",
    "Tools like pyvenv (see Item 53: “Use Virtual Environments for Isolated and Reproducible Dependencies”) make it easy to ensure that all environments have the same Python packages installed. The trouble is that production environments often require many external assumptions that are hard to reproduce in development environments.\n",
    "\n",
    "For example, say you want to run your program in a web server container and give it access to a database. This means that every time you want to modify your program’s code, you need to run a server container, the database must be set up properly, and your program needs the password for access. That’s a very high cost if all you’re trying to do is verify that a one-line change to your program works correctly.\n",
    "\n",
    "The best way to work around these issues is to override parts of your program at startup time to provide different functionality depending on the deployment environment. For example, you could have two different __main__ files, one for production and one for development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Programs often need to run in multiple deployment environments that each have unique assumptions and configurations.\n",
    "* You can tailor a module’s contents to different deployment environments by using normal Python statements in module scope.\n",
    "* Module contents can be the product of any external condition, including host introspection through the sys and os modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 55: Use repr Strings for Debugging Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "When debugging a Python program, the print function (or output via the logging built-in module) will get you surprisingly far. Python internals are often easy to access via plain attributes (see Item 27: “Prefer Public Attributes Over Private Ones”). All you need to do is print how the state of your program changes while it runs and see where it goes wrong.\n",
    "The print function outputs a human-readable string version of whatever you supply it. For example, printing a basic string will print the contents of the string without the surrounding quote characters.\n",
    "   print(‘foo bar’)\n",
    "   >>>\n",
    "   foo bar\n",
    "This is equivalent to using the '%s' format string and the % operator.\n",
    "   print(‘%s’ % ‘foo bar’)\n",
    "   >>>\n",
    "   foo bar\n",
    "The problem is that the human-readable string for a value doesn’t make it clear what the actual type of the value is. For example, notice how in the default output of print you can’t distinguish between the types of the number 5 and the string '5'.\n",
    "   print(5)\n",
    "   print(‘5’)\n",
    ">>> 5 5\n",
    "If you’re debugging a program with print, these type differences matter. What you almost always want while debugging is to see the repr version of an object. The repr built-in function returns the printable representation of an object, which should be its most clearly understandable string representation. For built-in types, the string returned by repr is a valid Python expression.\n",
    "￼￼￼￼￼\n",
    "a = ‘\\x07’\n",
    "   print(repr(a))\n",
    ">>> ‘\\x07’\n",
    "Passing the value from repr to the eval built-in function should result in the same Python object you started with (of course, in practice, you should only use eval with extreme caution).\n",
    "   b = eval(repr(a))\n",
    "   assert a == b\n",
    "When you’re debugging with print, you should repr the value before printing to ensure that any difference in types is clear.\n",
    "   print(repr(5))\n",
    "   print(repr(‘5’))\n",
    ">>> 5 ‘5’\n",
    "This is equivalent to using the '%r' format string and the % operator. print(‘%r’ % 5)\n",
    "   print(‘%r’ % ‘5’)\n",
    ">>> 5 ‘5’\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to Remember\n",
    "* Calling print on built-in Python types will produce the human-readable string version of a value, which hides type information.\n",
    "* Calling repr on built-in Python types will produce the printable string version of a value. These repr strings could be passed to the eval built-in function to get back the original value.\n",
    "* %s in format strings will produce human-readable strings like str. %r will produce printable strings like repr.\n",
    "* You can define the __repr__ method to customize the printable representation of a class and provide more detailed debugging information.\n",
    "* You can reach into any object’s __dict__ attribute to view its internals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 56: Test Everything with unittest\n",
    "\n",
    "* The only way to have confidence in a Python program is to write tests.\n",
    "* The unittest built-in module provides most of the facilities you’ll need to write good tests.\n",
    "* You can define tests by subclassing TestCase and defining one method per behavior you’d like to test. Test methods on TestCase classes must start with the word test.\n",
    "* It’s important to write both unit tests (for isolated functionality) and integration tests (for modules that interact).* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Item 57: Consider Interactive Debugging with pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everyone encounters bugs in their code while developing programs. Using the print function can help you track down the source of many issues (see Item 55: “Use repr Strings for Debugging Output”). Writing tests for specific cases that cause trouble is another great way to isolate problems (see Item 56: “Test Everything with unittest”).\n",
    "\n",
    "But these tools aren’t enough to find every root cause. When you need something more powerful, it’s time to try Python’s built-in interactive debugger. The debugger lets you inspect program state, print local variables, and step through a Python program one statement at a time.\n",
    "\n",
    "In most other programming languages, you use a debugger by specifying what line of a source file you’d like to stop on, then execute the program. In contrast, with Python the easiest way to use the debugger is by modifying your program to directly initiate the debugger just before you think you’ll have an issue worth investigating. There is no difference between running a Python program under a debugger and running it normally.\n",
    "\n",
    "To initiate the debugger, all you have to do is import the pdb built-in module and run its set_trace function. You’ll often see this done in a single line so programmers can comment it out with a single # character.\n",
    "Click here to view code image\n",
    "def complex_func(a, b, c): #...\n",
    "import pdb; pdb.set_trace()\n",
    "As soon as this statement runs, the program will pause its execution. The terminal that\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼\n",
    "started your program will turn into an interactive Python shell.\n",
    "Click here to view code image\n",
    "   -> import pdb; pdb.set_trace()\n",
    "   (Pdb)\n",
    "At the (Pdb) prompt, you can type in the name of local variables to see their values printed out. You can see a list of all local variables by calling the locals built-in function. You can import modules, inspect global state, construct new objects, run the help built-in function, and even modify parts of the program—whatever you need to do to aid in your debugging. In addition, the debugger has three commands that make inspecting the running program easier.\n",
    "\n",
    "bt: Print the traceback of the current execution call stack. This lets you figure out where you are in your program and how you arrived at the pdb.set_trace trigger point.\n",
    "up: Move your scope up the function call stack to the caller of the current function. This allows you to inspect the local variables in higher levels of the call stack.\n",
    "down: Move your scope back down the function call stack one level.\n",
    "Once you’re done inspecting the current state, you can use debugger commands to resume the program’s execution under precise control.\n",
    "\n",
    "step: Run the program until the next line of execution in the program, then return control back to the debugger. If the next line of execution includes calling a function, the debugger will stop in the function that was called.\n",
    "\n",
    "next: Run the program until the next line of execution in the current function, then return control back to the debugger. If the next line of execution includes calling a function, the debugger will not stop until the called function has returned.\n",
    "return: Run the program until the current function returns, then return control back to the debugger.\n",
    "continue: Continue running the program until the next breakpoint (or set_trace is called again).\n",
    "Things to Remember\n",
    "* You can initiate the Python interactive debugger at a point of interest directly in your program with the import pdb; pdb.set_trace() statements.\n",
    "* The Python debugger prompt is a full Python shell that lets you inspect and modify the state of a running program.\n",
    "* pdb shell commands let you precisely control program execution, allowing you to alternate between inspecting program state and progressing program execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gcd(a,b):\n",
    "    for i in range(min(a, b),0,-1):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        if a%i == 0 and b%i == 0:\n",
    "            return i\n",
    "print(gcd(3,99))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 58: Profile Before Optimizing\n",
    "\n",
    "* It’s important to profile Python programs before optimizing because the source of slowdowns is often obscure.\n",
    "* Use the cProfile module instead of the profile module because it provides more accurate profiling information.\n",
    "￼\n",
    "* The Profile object’s runcall method provides everything you need to profile a tree of function calls in isolation.\n",
    "* The Stats object lets you select and print the subset of \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item 59: Use tracemalloc to Understand Memory Usage and Leaks\n",
    "\n",
    "Memory management in the default implementation of Python, CPython, uses reference counting. This ensures that as soon as all references to an object have expired, the referenced object is also cleared. CPython also has a built-in cycle detector to ensure that self-referencing objects are eventually garbage collected.\n",
    "\n",
    "In theory, this means that most Python programmers don’t have to worry about allocating or deallocating memory in their programs. It’s taken care of automatically by the language and the CPython runtime. However, in practice, programs eventually do run out of memory due to held references. Figuring out where your Python programs are using or leaking memory proves to be a challenge.\n",
    "\n",
    "The first way to debug memory usage is to ask the gc built-in module to list every object currently known by the garbage collector. Although it’s quite a blunt tool, this approach does let you quickly get a sense of where your program’s memory is being used.\n",
    "\n",
    "Here, I run a program that wastes memory by keeping references. It prints out how many objects were created during execution and a small sample of allocated objects.\n",
    "Click here to view code image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import gc\n",
    "   found_objects = gc.get_objects()\n",
    "   print(‘%d objects before’ % len(found_objects))\n",
    "   import waste_memory\n",
    "   x = waste_memory.run()\n",
    "   found_objects = gc.get_objects()\n",
    "   print(‘%d objects after’ % len(found_objects))\n",
    "   for obj in found_objects[:3]:\n",
    "       print(repr(obj)[:100])\n",
    "```\n",
    "\n",
    "* It can be difficult to understand how Python programs use and leak memory.\n",
    "* The gc module can help you understand which objects exist, but it has no information about how they were allocated.\n",
    "* The tracemalloc built-in module provides powerful tools for understanding the source of memory usage.\n",
    "* tracemalloc is only available in Python 3.4 and above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
